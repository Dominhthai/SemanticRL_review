{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:17.807581Z",
     "iopub.status.busy": "2025-08-12T11:14:17.807320Z",
     "iopub.status.idle": "2025-08-12T11:14:19.236987Z",
     "shell.execute_reply": "2025-08-12T11:14:19.236210Z",
     "shell.execute_reply.started": "2025-08-12T11:14:17.807553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SemanticRL'...\n",
      "remote: Enumerating objects: 158, done.\u001b[K\n",
      "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
      "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
      "remote: Total 158 (delta 40), reused 114 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (158/158), 461.14 KiB | 3.12 MiB/s, done.\n",
      "Resolving deltas: 100% (40/40), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lukun199/SemanticRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:19.239432Z",
     "iopub.status.busy": "2025-08-12T11:14:19.239080Z",
     "iopub.status.idle": "2025-08-12T11:14:19.246377Z",
     "shell.execute_reply": "2025-08-12T11:14:19.245610Z",
     "shell.execute_reply.started": "2025-08-12T11:14:19.239387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SemanticRL\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/SemanticRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:19.247489Z",
     "iopub.status.busy": "2025-08-12T11:14:19.247234Z",
     "iopub.status.idle": "2025-08-12T11:14:30.482901Z",
     "shell.execute_reply": "2025-08-12T11:14:30.482244Z",
     "shell.execute_reply.started": "2025-08-12T11:14:19.247466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.5.2 (from -r requirements.txt (line 1))\n",
      "  Downloading matplotlib-3.5.2.tar.gz (35.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.6\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:30.484672Z",
     "iopub.status.busy": "2025-08-12T11:14:30.483812Z",
     "iopub.status.idle": "2025-08-12T11:14:38.039874Z",
     "shell.execute_reply": "2025-08-12T11:14:38.039119Z",
     "shell.execute_reply.started": "2025-08-12T11:14:30.484643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/european-parliament/dataset /kaggle/working/SemanticRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:38.041915Z",
     "iopub.status.busy": "2025-08-12T11:14:38.041717Z",
     "iopub.status.idle": "2025-08-12T11:14:38.047582Z",
     "shell.execute_reply": "2025-08-12T11:14:38.046933Z",
     "shell.execute_reply.started": "2025-08-12T11:14:38.041895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SemanticRL/dataset\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/SemanticRL/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:38.048569Z",
     "iopub.status.busy": "2025-08-12T11:14:38.048247Z",
     "iopub.status.idle": "2025-08-12T11:14:43.110436Z",
     "shell.execute_reply": "2025-08-12T11:14:43.109480Z",
     "shell.execute_reply.started": "2025-08-12T11:14:38.048542Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf fr-en.tgz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:14:43.111913Z",
     "iopub.status.busy": "2025-08-12T11:14:43.111631Z",
     "iopub.status.idle": "2025-08-12T11:15:45.551750Z",
     "shell.execute_reply": "2025-08-12T11:15:45.550839Z",
     "shell.execute_reply.started": "2025-08-12T11:14:43.111879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data: sentences=2007723, min=0, max=668\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "Saved: english.pkl\n",
      "resumption of the session\n",
      "i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
      "although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
      "you have requested a debate on this subject in the course of the next few days during this partsession\n",
      "in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n",
      "please rise then for this minute s silence\n",
      "the house rose and observed a minute s silence\n",
      "madam president on a point of order\n",
      "you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n",
      "one of the people assassinated very recently in sri lanka was mr kumar ponnambalam who had visited the european parliament just a few months ago\n",
      "English Vocabulary: 59616\n",
      "New English Vocabulary: 24060\n",
      "Saved: english_vocab.pkl\n",
      "resumption of the session\n",
      "you have requested a debate on this subject in the course of the next few days during this partsession\n",
      "please rise then for this minute s silence\n",
      "the house rose and observed a minute s silence\n",
      "madam president on a point of order\n",
      "yes mr evans i feel an initiative of the type you have just suggested would be entirely appropriate\n",
      "if the house agrees i shall do as mr evans has suggested\n",
      "madam president on a point of order\n",
      "i would like your advice about rule concerning inadmissibility\n",
      "my question relates to something that will come up on thursday and which i will then raise again\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/SemanticRL/preprocess_captions.py --data_root /kaggle/working/SemanticRL/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SemanticRL-JSCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:20:01.449439Z",
     "iopub.status.busy": "2025-08-12T11:20:01.449146Z",
     "iopub.status.idle": "2025-08-12T11:20:01.457354Z",
     "shell.execute_reply": "2025-08-12T11:20:01.456507Z",
     "shell.execute_reply.started": "2025-08-12T11:20:01.449413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/utils.py\n",
    "\"\"\"\n",
    "lukun199@gmail.com\n",
    "3rd Feb., 2021\n",
    "\n",
    "# utils.py\n",
    "\"\"\"\n",
    "import os, sys, time, glob, shutil\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Communication Utils: Channel Model, error rate, etc.\n",
    "\n",
    "class Normlize_tx:\n",
    "    def __init__(self, _iscomplex):\n",
    "        self._iscomplex = _iscomplex\n",
    "    def apply(self, _input):\n",
    "        _dim = _input.shape[1]//2 if self._iscomplex else _input.shape[1]\n",
    "        _norm = _dim**0.5 / torch.sqrt(torch.sum(_input**2, dim=1))\n",
    "        return _input*_norm.view(-1,1)\n",
    "\n",
    "class Channel:\n",
    "    # returns the message when passed through a channel.\n",
    "    # AGWN, Fading\n",
    "    # Note that we need to make sure that the colle map will not change in this\n",
    "    # step, thus we should not use *= and +=.\n",
    "    def __init__(self, _iscomplex):\n",
    "        self._iscomplex = _iscomplex\n",
    "\n",
    "    def ideal_channel(self, _input):\n",
    "        return _input\n",
    "\n",
    "    def awgn(self, _input, _snr):\n",
    "        _std = (10**(-_snr/10.)/2)**0.5 if self._iscomplex else (10**(-_snr/10.))**0.5  # for complex signals.\n",
    "        _input = _input + torch.randn_like(_input) * _std\n",
    "        #print(_std)\n",
    "        return _input\n",
    "\n",
    "    def awgn_physical_layer(self, _input, _snr):\n",
    "        _std = (10**(-_snr/10.)/2)**0.5\n",
    "        _input = _input + torch.randn_like(_input) * _std\n",
    "        #print(_std)\n",
    "        return _input\n",
    "\n",
    "    def fading(self, _input, _snr):\n",
    "        # ref from DeepJSCC-f https://github.com/kurka/deepJSCC-feedback\n",
    "        if self._iscomplex:\n",
    "            _shape = _input.shape\n",
    "            _dim = _shape[1]//2\n",
    "            _std = (10**(-_snr/10.)/2)**0.5\n",
    "            _mul = torch.abs(torch.randn(_shape[0], 2)/(2**0.5))  # should divide 2**0.5 here.\n",
    "            _input_ = _input.clone()\n",
    "            _input_[:,:_dim] *= _mul[:,0].view(-1,1)\n",
    "            _input_[:,_dim:] *= _mul[:,1].view(-1,1)\n",
    "            _input = _input_\n",
    "        else:\n",
    "            _std = (10**(-_snr/10.))**0.5\n",
    "            _input = _input * torch.abs(torch.randn(_input.shape[0], 1)).to(_input)\n",
    "        _input = _input + torch.randn_like(_input) * _std\n",
    "        #print(_std)\n",
    "        return _input\n",
    "\n",
    "    def phase_invariant_fading(self, _input, _snr):\n",
    "        # ref from DeepJSCC-f\n",
    "        _std = (10**(-_snr/10.)/2)**0.5 if self._iscomplex else (10**(-_snr/10.))**0.5\n",
    "        if self._iscomplex:\n",
    "            _mul = (torch.randn(_input.shape[0], 1)**2/2. + torch.randn(_input.shape[0], 1)**2/2.)**0.5\n",
    "        else:\n",
    "            _mul = (torch.randn(_input.shape[0], 1)**2 + torch.randn(_input.shape[0], 1)**2) ** 0.5\n",
    "        _input = _input * _mul.to(_input)\n",
    "        _input = _input +  torch.randn_like(_input) * _std\n",
    "        #print(_std)\n",
    "        return _input\n",
    "\n",
    "# Other Utils:\n",
    "\n",
    "class Crit:\n",
    "\n",
    "    def __call__(self, mode, *args):\n",
    "        return getattr(self, '_' + mode)(*args)\n",
    "\n",
    "    def _ce(self, pred, target, lengths):\n",
    "        mask = pred.new_zeros(len(lengths), target.size(1))\n",
    "        for i, l in enumerate(lengths): # length can be bigger than the true len\n",
    "            mask[i, :l] = 1\n",
    "\n",
    "        loss = - pred.gather(2, target.unsqueeze(2)).squeeze(2) * mask   # log counted.\n",
    "        loss = torch.sum(loss) / torch.sum(mask)  # ln(vocab_dim=24064) is around 10\n",
    "        return loss\n",
    "\n",
    "    def _rl(self, seq_logprobs, seq_masks, reward):\n",
    "        # seq_logprobs[bs, T] reward:[bs, 1] or [bs, T]\n",
    "        output = - seq_logprobs * seq_masks * reward\n",
    "        output = torch.sum(output) / torch.sum(seq_masks)\n",
    "        return output\n",
    "\n",
    "    def _tx_gaussian_sample(self, log_samples, reward):\n",
    "        return -(log_samples*reward).mean()\n",
    "\n",
    "# SCSIU\n",
    "class GaussianPolicy():\n",
    "\n",
    "    def forward(self, x, std=0.1):\n",
    "        return x + torch.randn_like(x) * std\n",
    "\n",
    "    def forward_sample(self, mean, std=0.1):\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        ln_prob = dist.log_prob(action)\n",
    "        return action, ln_prob\n",
    "\n",
    "# others\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def time_consum_wrapper(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        print(func.__name__, 'is running')\n",
    "        res = func(*args, **kwargs)\n",
    "        print(\"time func takes\", time.time() - start)\n",
    "        return res\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def smaple_n_times(n, x):\n",
    "        if n>1:\n",
    "            x = x.unsqueeze(1) # Bx1x...\n",
    "            x = x.expand(-1, n, *([-1]*len(x.shape[2:])))\n",
    "            x = x.reshape(x.shape[0]*n, *x.shape[2:])\n",
    "        return x\n",
    "\n",
    "def copyStage1ckpts(frompath, topath, strs='resume_from_ce_'):\n",
    "    os.makedirs(topath, exist_ok=True)\n",
    "    files = glob.glob(os.path.join(frompath, '*24.pth'))\n",
    "    for file in files:\n",
    "        shutil.copyfile(file, os.path.join(topath, strs+os.path.basename(file)))\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    is_complex = False\n",
    "    n = Normlize_tx(is_complex)\n",
    "    x = torch.tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9., 10.], [18., 2., 3., 4., 5., 6., 7., 8., 9., 10.]])\n",
    "    y = n.apply(x)\n",
    "    print(y)\n",
    "    #for i in range(x.shape[1]//2):\n",
    "    #    print(y[:,i], y[:,5+i])\n",
    "\n",
    "    c = Channel(is_complex)\n",
    "    # x = torch.ones(2,4)\n",
    "    z = c.awgn(y,10)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:26.251853Z",
     "iopub.status.busy": "2025-08-04T06:15:26.251612Z",
     "iopub.status.idle": "2025-08-04T06:15:26.271954Z",
     "shell.execute_reply": "2025-08-04T06:15:26.271225Z",
     "shell.execute_reply.started": "2025-08-04T06:15:26.251830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/config/config_AWGN_CE.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/config/config_AWGN_CE.yaml\n",
    "snr: 10\n",
    "multiple_sample: 5\n",
    "scheduled_sampling_start: 18\n",
    "iscomplex: 1\n",
    "channel_type: 'awgn'\n",
    "batch_size: 64\n",
    "channel_dim: 256\n",
    "ckpt_resume: -1\n",
    "lr_milestones: '20 88'\n",
    "max_epoch: 25\n",
    "seeds: 7\n",
    "init_learning_rate: 1e-3\n",
    "save_model_path: './ckpt_AWGN_CE/'\n",
    "RL_training: 0\n",
    "device: 'cuda:0'\n",
    "backbone: 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:26.273004Z",
     "iopub.status.busy": "2025-08-04T06:15:26.272779Z",
     "iopub.status.idle": "2025-08-04T06:15:26.288185Z",
     "shell.execute_reply": "2025-08-04T06:15:26.287674Z",
     "shell.execute_reply.started": "2025-08-04T06:15:26.272977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/config/config_AWGN_CE_Stage2.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/config/config_AWGN_CE_Stage2.yaml\n",
    "snr: 10\n",
    "multiple_sample: 5\n",
    "scheduled_sampling_start: 18\n",
    "iscomplex: 1\n",
    "channel_type: 'awgn'\n",
    "batch_size: 64\n",
    "channel_dim: 256\n",
    "ckpt_resume: 25\n",
    "lr_milestones: '160'\n",
    "max_epoch: 75\n",
    "seeds: 7\n",
    "init_learning_rate: 1e-4\n",
    "save_model_path: './ckpt_AWGN_CE_Stage2/'\n",
    "RL_training: 0\n",
    "device: 'cuda:0'\n",
    "backbone: 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:26.289100Z",
     "iopub.status.busy": "2025-08-04T06:15:26.288882Z",
     "iopub.status.idle": "2025-08-04T06:15:26.308429Z",
     "shell.execute_reply": "2025-08-04T06:15:26.307707Z",
     "shell.execute_reply.started": "2025-08-04T06:15:26.289076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SemanticRL/dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:22:46.396232Z",
     "iopub.status.busy": "2025-08-12T11:22:46.395535Z",
     "iopub.status.idle": "2025-08-12T11:22:46.402176Z",
     "shell.execute_reply": "2025-08-12T11:22:46.401309Z",
     "shell.execute_reply.started": "2025-08-12T11:22:46.396202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SemanticRL\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/SemanticRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:26.325310Z",
     "iopub.status.busy": "2025-08-04T06:15:26.324858Z",
     "iopub.status.idle": "2025-08-04T06:15:29.769662Z",
     "shell.execute_reply": "2025-08-04T06:15:29.768876Z",
     "shell.execute_reply.started": "2025-08-04T06:15:26.325286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/ckpt_awgn_ce/pytorch/default/1 /kaggle/working/SemanticRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:29.770939Z",
     "iopub.status.busy": "2025-08-04T06:15:29.770686Z",
     "iopub.status.idle": "2025-08-04T06:15:29.775198Z",
     "shell.execute_reply": "2025-08-04T06:15:29.774486Z",
     "shell.execute_reply.started": "2025-08-04T06:15:29.770905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "old_folder_path = \"/kaggle/working/SemanticRL/1\"\n",
    "new_folder_path = \"/kaggle/working/SemanticRL/ckpt_AWGN_CE\"\n",
    "os.rename(old_folder_path, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:20:18.435799Z",
     "iopub.status.busy": "2025-08-12T11:20:18.435361Z",
     "iopub.status.idle": "2025-08-12T11:20:18.443386Z",
     "shell.execute_reply": "2025-08-12T11:20:18.442669Z",
     "shell.execute_reply.started": "2025-08-12T11:20:18.435778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/self_critical/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/self_critical/utils.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import os\n",
    "from .cider.pyciderevalcap.ciderD.ciderD import CiderD\n",
    "from .bleu.bleu import Bleu\n",
    "\n",
    "\n",
    "def write2txt(fp, info, mode='a'):\n",
    "    \"\"\"\n",
    "    Write information to a text file\n",
    "    \n",
    "    Args:\n",
    "        fp: file path\n",
    "        info: information to write\n",
    "        mode: file mode ('a' for append, 'w' for write)\n",
    "    \"\"\"\n",
    "    with open(fp, mode=mode) as f:\n",
    "        f.write(info)\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def _array_to_str(arr, sos_token, eos_token):\n",
    "    out = ''\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] == eos_token:\n",
    "            break\n",
    "        out += str(arr[i]) + ' '\n",
    "    out += str(eos_token)  # optional\n",
    "    return out.strip()\n",
    "\n",
    "\n",
    "def get_4_bleu_score(output, ground_truth, reward_scorer, sos_token=1, eos_token=2):\n",
    "    \"\"\"\n",
    "    Calculate BLEU-1,2,3,4 scores for CE baseline evaluation\n",
    "    \n",
    "    Args:\n",
    "        output: decoder output predictions [batch_size, seq_len, vocab_size]\n",
    "        ground_truth: ground truth sentences [batch_size, seq_len]\n",
    "        reward_scorer: BLEU scorer instance\n",
    "        sos_token: start of sentence token (default=1)\n",
    "        eos_token: end of sentence token (default=2)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (bleu1_score, bleu2_score, bleu3_score, bleu4_score)\n",
    "    \"\"\"\n",
    "    device = output.device\n",
    "    batch_size = output.size(0)\n",
    "    \n",
    "    # Convert output probabilities to predicted tokens\n",
    "    predicted_tokens = torch.argmax(output, dim=-1)  # [batch_size, seq_len]\n",
    "    \n",
    "    # Convert to numpy for evaluation\n",
    "    predicted_tokens = predicted_tokens.cpu().numpy()\n",
    "    ground_truth = ground_truth.cpu().numpy()\n",
    "    \n",
    "    # Prepare data for BLEU calculation\n",
    "    sample_result = []\n",
    "    gts = {}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Convert predicted tokens to string\n",
    "        pred_str = _array_to_str(predicted_tokens[i], sos_token, eos_token)\n",
    "        sample_result.append({'image_id': i, 'caption': [pred_str]})\n",
    "        \n",
    "        # Convert ground truth to string\n",
    "        gt_str = _array_to_str(ground_truth[i], sos_token, eos_token)\n",
    "        gts[i] = [gt_str]\n",
    "    \n",
    "    # Calculate BLEU scores using the reward_scorer\n",
    "    if isinstance(reward_scorer, Bleu):\n",
    "        _, scores_mat = reward_scorer.compute_score(gts, sample_result)\n",
    "        \n",
    "        # Extract individual BLEU scores\n",
    "        scores_b1 = np.array(scores_mat[0]).mean()\n",
    "        scores_b2 = np.array(scores_mat[1]).mean()\n",
    "        scores_b3 = np.array(scores_mat[2]).mean()\n",
    "        scores_b4 = np.array(scores_mat[3]).mean()\n",
    "        \n",
    "        return scores_b1, scores_b2, scores_b3, scores_b4\n",
    "    else:\n",
    "        print(\"Warning: reward_scorer is not a BLEU scorer\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "def log_training_metrics(epoch, loss, bleu1, bleu2, bleu3, bleu4, log_path):\n",
    "    \"\"\"\n",
    "    Log training metrics to text file\n",
    "    \n",
    "    Args:\n",
    "        epoch: current epoch\n",
    "        loss: training loss\n",
    "        bleu1, bleu2, bleu3, bleu4: BLEU scores\n",
    "        log_path: path to log file\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    log_dir = os.path.dirname(log_path)\n",
    "    if log_dir and not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    info = f'epoch:{epoch} loss:{loss:.6f} bleu1:{bleu1:.4f} bleu2:{bleu2:.4f} bleu3:{bleu3:.4f} bleu4:{bleu4:.4f}'\n",
    "    write2txt(log_path, info)\n",
    "\n",
    "\n",
    "def get_ciderd_scorer_europarl(split_captions, test_data_num, sos_token, eos_token):\n",
    "\n",
    "    all_caps = np.concatenate((split_captions, test_data_num))\n",
    "    print('====> get_ciderd_scorer begin, seeing {} sentences'.format(len(all_caps)))\n",
    "\n",
    "    refs_idxs = []\n",
    "    for caps in all_caps:\n",
    "        ref_idxs = []\n",
    "        ref_idxs.append(_array_to_str(caps, sos_token, eos_token))\n",
    "        refs_idxs.append(ref_idxs)\n",
    "\n",
    "    scorer = CiderD(refs_idxs)\n",
    "    del refs_idxs\n",
    "    del ref_idxs\n",
    "    print('====> get_ciderd_scorer end')\n",
    "    return scorer\n",
    "\n",
    "def get_bleu_scorer_europarl(n=4):\n",
    "\n",
    "    scorer = Bleu(n=n)\n",
    "    print('====> get_bleu_scorer end')\n",
    "\n",
    "    return scorer\n",
    "\n",
    "\n",
    "def get_self_critical_reward_sc(sample_captions, fns, ground_truth,\n",
    "                             sos_token, eos_token, scorer):\n",
    "    # the first dim of fns are the same with samples. fns is a list.\n",
    "    device = sample_captions.device\n",
    "    batch_size = len(ground_truth)\n",
    "    seq_per_img = len(fns) // batch_size\n",
    "    sample_captions = sample_captions.cpu().numpy()\n",
    "    ground_truth = ground_truth.cpu().numpy()\n",
    "\n",
    "    max_seq_len = sample_captions.shape[1]\n",
    "    sample_result = []\n",
    "    gts = {}\n",
    "    # first multiple samples.\n",
    "    for fn in fns:\n",
    "        sample_result.append({'image_id': fn, 'caption': [_array_to_str(sample_captions[fn], sos_token, eos_token)]})\n",
    "        caps = []\n",
    "        caps.append(_array_to_str(ground_truth[fn//seq_per_img][:max_seq_len], sos_token, eos_token))\n",
    "        gts[fn] = caps\n",
    "\n",
    "    if isinstance(scorer, CiderD):\n",
    "        _, scores = scorer.compute_score(gts, sample_result)  # [bs*5,1]\n",
    "        scores = torch.from_numpy(scores).to(device).view(-1, seq_per_img)  # [bs,5]\n",
    "        detailed_reward = None\n",
    "    elif isinstance(scorer, Bleu):\n",
    "        _, scores_mat = scorer.compute_score(gts, sample_result)\n",
    "        scores_b1 = np.array(scores_mat[0]).mean()\n",
    "        scores_b2 = np.array(scores_mat[1]).mean()\n",
    "        scores_b3 = np.array(scores_mat[2]).mean()\n",
    "        scores_b4 = np.array(scores_mat[3]).mean()\n",
    "        detailed_reward = (scores_b1, scores_b2, scores_b3, scores_b4)\n",
    "        scores = (np.array(scores_mat[0]) + np.array(scores_mat[3]))/2\n",
    "        scores = torch.from_numpy(scores).to(device).view(-1, seq_per_img)  # [bs,5]\n",
    "\n",
    "    scores.requires_grad = False\n",
    "    baseline = (scores.sum(1, keepdim=True) - scores) / (scores.shape[1] - 1) # [bs,5]\n",
    "    scores = scores - baseline # [bs,5]\n",
    "    scores = scores.view(-1, 1)  # [bs*5, 1]\n",
    "    return scores, baseline.mean(), detailed_reward\n",
    "\n",
    "\n",
    "def get_self_critical_reward_newsc_TXRL(sample_captions, fns, ground_truth,\n",
    "                                        sos_token, eos_token, scorer):\n",
    "    # the first dim of fns are the same with samples. fns is a list.\n",
    "    device = sample_captions.device\n",
    "    batch_size = len(ground_truth)\n",
    "    seq_per_img = len(fns) // batch_size\n",
    "    sample_captions = sample_captions.cpu().numpy()\n",
    "    ground_truth = ground_truth.cpu().numpy()\n",
    "\n",
    "    max_seq_len = sample_captions.shape[1]\n",
    "    sample_result = []\n",
    "    gts = {}\n",
    "    # first multiple samples.\n",
    "    for fn in fns:\n",
    "        sample_result.append({'image_id': fn, 'caption': [_array_to_str(sample_captions[fn], sos_token, eos_token)]})\n",
    "        caps = []\n",
    "        caps.append(_array_to_str(ground_truth[fn // seq_per_img][:max_seq_len], sos_token, eos_token))\n",
    "        gts[fn] = caps\n",
    "\n",
    "    _, scores = scorer.compute_score(gts, sample_result)  # [bs*5,1]\n",
    "    scores = torch.from_numpy(scores).to(device).view(-1, seq_per_img)  # [bs,5]\n",
    "    scores.requires_grad = False\n",
    "    if seq_per_img > 1:\n",
    "        baseline = (scores.sum(1, keepdim=True) - scores) / (scores.shape[1] - 1)  # [bs,5]\n",
    "        scores = scores - baseline  # [bs,5]\n",
    "    scores = scores.view(-1, 1)  # [bs*5, 1]\n",
    "\n",
    "    if seq_per_img > 1:\n",
    "        return scores, baseline.mean()\n",
    "    else:\n",
    "        return scores, scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:20:22.934992Z",
     "iopub.status.busy": "2025-08-12T11:20:22.934124Z",
     "iopub.status.idle": "2025-08-12T11:20:22.946672Z",
     "shell.execute_reply": "2025-08-12T11:20:22.946094Z",
     "shell.execute_reply.started": "2025-08-12T11:20:22.934969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/Trainng_SemanticRL.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/Trainng_SemanticRL.py\n",
    "\"\"\"\n",
    "lukun199@gmail.com\n",
    "18th Feb., 2021\n",
    "\n",
    "# train.py\n",
    "\"\"\"\n",
    "import os, argparse, yaml, random\n",
    "import platform\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from data_loader import Dataset_sentence, collate_func\n",
    "from model import get_model\n",
    "from utils import Normlize_tx, Channel, Crit, clip_gradient, copyStage1ckpts, smaple_n_times, GaussianPolicy\n",
    "from self_critical.utils import get_ciderd_scorer_europarl, get_bleu_scorer_europarl, get_self_critical_reward_sc, get_self_critical_reward_newsc_TXRL, get_4_bleu_score, log_training_metrics\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--snr\", type=int, default=10)\n",
    "parser.add_argument(\"--multiple_sample\", type=int, default=5)  # number of parallel samples\n",
    "parser.add_argument(\"--scheduled_sampling_start\", type=int, default=18)  # when to start scheduled sampling\n",
    "parser.add_argument(\"--iscomplex\", type=float, default=1)\n",
    "parser.add_argument(\"--channel_type\", type=str, default='awgn')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--channel_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--ckpt_resume\", type=int, default=-1)\n",
    "parser.add_argument(\"--lr_milestones\", type=str, default='160') # update the learning rate\n",
    "parser.add_argument(\"--max_epoch\", type=int, default=202)\n",
    "parser.add_argument(\"--seeds\", type=int, default=7)  # not used in the paper. but we can assign it\n",
    "parser.add_argument(\"--init_learning_rate\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--save_model_path\", type=str, default=\"./ckpt_RL/\")\n",
    "parser.add_argument(\"--dataset_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--RL_training\", type=float, default=1)\n",
    "parser.add_argument(\"--reward_type\", type=str, default='CIDEr')  # BLEU, CIDEr\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda:0\")\n",
    "parser.add_argument(\"--backbone\", type=str, default=\"LSTM\")  # 'Transformer' 'LSTM'\n",
    "parser.add_argument(\"--training_config\", type=str, default=\"\")\n",
    "parser.add_argument(\"--teacher_forcing\", type=float, default=0)\n",
    "parser.add_argument(\"--SemanticRL_JSCC\", type=float, default=1)  # set 0 for SemanticRL-SCSIU\n",
    "parser.add_argument(\"--accumulate_grad\", type=float, default=20)  # for variant SCSIU\n",
    "parser.add_argument(\"--log_bleu\", action='store_true', help='log BLEU scores during CE training')\n",
    "parser.add_argument(\"--log_file\", type=str, default=\"./results/training_log.txt\", help='path to log file')\n",
    "args = parser.parse_args()\n",
    "if args.training_config:\n",
    "    f = yaml.safe_load(open(args.training_config, 'r'))\n",
    "    for kk, vv in f.items():\n",
    "        setattr(args, kk, vv)\n",
    "setattr(args, 'init_learning_rate', float(args.init_learning_rate))\n",
    "assert args.init_learning_rate<1, 'please check if your learning rate < 1'\n",
    "\n",
    "\n",
    "# seeds\n",
    "def init_seeds(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "init_seeds(args.seeds)\n",
    "\n",
    "print('\\n[*]---------------args init done')\n",
    "print('args preview:')\n",
    "for k in args.__dict__:  print(k + \": \" + str(args.__dict__[k]))\n",
    "print('[*]---------------starting preparing dataset and network. Preparing the dataset may take a few minutes')\n",
    "\n",
    "os.makedirs(args.save_model_path, exist_ok=True)\n",
    "device = torch.device(args.device)\n",
    "opt_decoder = 1\n",
    "train_loader_params = {'batch_size': args.batch_size,\n",
    "                       'shuffle': True, 'num_workers':4,  # set to 0 in Windows system\n",
    "                       'collate_fn': lambda x: collate_func(x),\n",
    "                       'drop_last': True}  # setting as False should also be OK\n",
    "data_train = Dataset_sentence(_path = args.dataset_path, use_sos=args.backbone=='Transformer')\n",
    "train_data_loader = data.DataLoader(data_train,**train_loader_params)\n",
    "\n",
    "\n",
    "embeds_shared = get_model('Embeds')(vocab_size = data_train.get_dict_len(), num_hidden=128).to(device)\n",
    "encoder = get_model(args.backbone+'Encoder')(channel_dim=args.channel_dim, embedds=embeds_shared).to(device)\n",
    "decoder = get_model(args.backbone+'Decoder')(channel_dim=args.channel_dim, embedds=embeds_shared,\n",
    "                             vocab_size=data_train.get_dict_len()).to(device)\n",
    "normlize_layer = Normlize_tx(_iscomplex=args.iscomplex)\n",
    "channel = Channel(_iscomplex=args.iscomplex)\n",
    "policy = GaussianPolicy()\n",
    "\n",
    "# print #params             a+b-c since embeds_shared is contained in both TX and RX\n",
    "nums_model = sum(x.numel() for x in encoder.parameters() if x.requires_grad is True) + \\\n",
    "             sum(x.numel() for x in encoder.parameters() if x.requires_grad is True) - \\\n",
    "             sum(x.numel() for x in embeds_shared.parameters() if x.requires_grad is True)\n",
    "print(\"Model {} have {} paramerters in total\".format(args.backbone, nums_model))\n",
    "\n",
    "# load saved ckpt\n",
    "if args.ckpt_resume>0:\n",
    "    copyStage1ckpts('./ckpt_{}_CE'.format('AWGN' if args.channel_type=='awgn' else 'FIF'), args.save_model_path)\n",
    "    embeds_shared.load_state_dict(\n",
    "        torch.load(args.save_model_path + 'resume_from_ce_embeds_shared_epoch{}.pth'.format(args.ckpt_resume - 1)))\n",
    "    encoder.load_state_dict(torch.load(args.save_model_path + 'resume_from_ce_encoder_epoch{}.pth'.format(args.ckpt_resume-1)))\n",
    "    decoder.load_state_dict(torch.load(args.save_model_path + 'resume_from_ce_decoder_epoch{}.pth'.format(args.ckpt_resume-1)))\n",
    "    print('[*]---------------loaded ckpt at' + args.save_model_path)\n",
    "\n",
    "\n",
    "# multigpu training with ddp brings a performance degradation. this is perhaps caused by the random number in the channel.\n",
    "# so we will not provide the ddp version\n",
    "\n",
    "_params = list(list(embeds_shared.parameters()) + list(decoder.parameters()) + list(encoder.parameters()))\n",
    "if args.SemanticRL_JSCC:\n",
    "    optimizer = torch.optim.Adam(_params, lr=args.init_learning_rate)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                           milestones=[int(x) for x in args.lr_milestones.split(' ')],\n",
    "                                           gamma=0.5)\n",
    "else:  # SemanticRL-SCSIU\n",
    "    # we assign the shared embedding to decoder. this is not a must.\n",
    "    optimizer_encoder = torch.optim.Adam(list(set(list(encoder.parameters())) - set(list(embeds_shared.parameters()))),\n",
    "                                         lr=args.init_learning_rate)\n",
    "    optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=args.init_learning_rate)\n",
    "    scheduler_encoder = optim.lr_scheduler.MultiStepLR(optimizer_encoder,\n",
    "                                                       milestones=[int(x) for x in args.lr_milestones.split(' ')],\n",
    "                                                       gamma=0.5)\n",
    "    scheduler_decoder = optim.lr_scheduler.MultiStepLR(optimizer_decoder,\n",
    "                                                       milestones=[int(x) for x in args.lr_milestones.split(' ')],\n",
    "                                                       gamma=0.5)\n",
    "\n",
    "\n",
    "# loss function\n",
    "crit = Crit()\n",
    "print('[*]---------------network config done.')\n",
    "\n",
    "# reward config for RL training\n",
    "if args.RL_training:\n",
    "    reward_scorer = get_ciderd_scorer_europarl(data_train.data_num.numpy(), np.array(data_train.test_data_num),\n",
    "                                               sos_token=1, eos_token=2) if args.reward_type == 'CIDEr' \\\n",
    "                    else get_bleu_scorer_europarl()\n",
    "else:\n",
    "    # For CE baseline, we also need BLEU scorer for evaluation\n",
    "    reward_scorer = get_bleu_scorer_europarl()\n",
    "\n",
    "\n",
    "def train(encoder, decoder, device, train_loader, optimizer, epoch):\n",
    "\n",
    "    # set model as training mode\n",
    "    embeds_shared.train()\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    print('--------------------epoch: %d' % epoch)\n",
    "\n",
    "    # scheduled sampling\n",
    "    frac = (epoch - args.scheduled_sampling_start) // 7  # hyper params. you can set to others\n",
    "    ss_prob = min(0.05 * frac, 0.25)\n",
    "    if ss_prob<0: ss_prob=0  # avoid ss_prob<0\n",
    "\n",
    "    # training loops\n",
    "    total_loss = 0\n",
    "    total_bleu1, total_bleu2, total_bleu3, total_bleu4 = 0, 0, 0, 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (train_sents, len_batch) in enumerate(train_loader):\n",
    "        # Note, for LSTM we feed m=[w_1, w_2, w_3, ...]\n",
    "        # for Transformer we feed m=[<SOS>, w_1, w_2, w_3, ...]\n",
    "\n",
    "        # distribute data to device\n",
    "        train_sents = train_sents.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, src_mask = encoder(train_sents, len_batch)  # for both LSTM and Transformer\n",
    "        output = normlize_layer.apply(output)\n",
    "        output = getattr(channel, args.channel_type)(output, _snr=args.snr)\n",
    "        if not args.RL_training:\n",
    "            output = decoder.forward_ce(output, train_sents, src_mask, ss_prob)\n",
    "            loss = crit('ce', output, train_sents if 'LSTM' in args.backbone else train_sents[:,1:],\n",
    "                        len_batch if 'LSTM' in args.backbone else [x-1 for x in len_batch])  # remove sos.\n",
    "            \n",
    "            # Calculate BLEU scores for CE baseline if logging is enabled\n",
    "            if (args.log_bleu or not args.RL_training) and batch_idx % 100 == 0:  # Calculate BLEU every 100 batches to save time\n",
    "                try:\n",
    "                    bleu1, bleu2, bleu3, bleu4 = get_4_bleu_score(\n",
    "                        output, \n",
    "                        train_sents if 'LSTM' in args.backbone else train_sents[:,1:],\n",
    "                        reward_scorer\n",
    "                    )\n",
    "                    total_bleu1 += bleu1\n",
    "                    total_bleu2 += bleu2\n",
    "                    total_bleu3 += bleu3\n",
    "                    total_bleu4 += bleu4\n",
    "                    num_batches += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating BLEU: {e}\")\n",
    "        else:\n",
    "            if args.teacher_forcing>0:\n",
    "                sample_captions, sample_logprobs, seq_masks = decoder.forward_rl_ssprob(output, train_sents,\n",
    "                                 sample_max=False, multiple_sample=args.multiple_sample,  x_mask=src_mask)\n",
    "\n",
    "            else:\n",
    "                # decoder.sample_max_batch(output, src_mask) # for inference only\n",
    "                sample_captions, sample_logprobs, seq_masks = decoder.forward_rl(output, sample_max=False,\n",
    "                                                     multiple_sample=args.multiple_sample, x_mask=src_mask)\n",
    "            fns = list(range(sample_logprobs.size()[0]))\n",
    "            advantage, reward_mean, detailed_reward = get_self_critical_reward_sc(sample_captions,\n",
    "                fns, train_sents if 'LSTM' in args.backbone else train_sents[:,1:], 1, 2, reward_scorer)\n",
    "            loss = crit('rl', sample_logprobs, seq_masks, advantage)\n",
    "\n",
    "        # When using Transformer backbone, it is recommended to set args.set teacher_forcing >0, and combine CE and RL loss to \n",
    "        # get the best performance. i.e., `loss = loss_ce*lambda + loss_rl` where lambda can be, say, 0.2.\n",
    "        loss.backward()\n",
    "        clip_gradient(optimizer, 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx%500==0:\n",
    "            if not args.RL_training:\n",
    "                print('[%4d / %4d]    '%(batch_idx, epoch) , '    loss = ', loss.item())\n",
    "            else:\n",
    "                print('[%4d / %4d]    ' % (batch_idx, epoch), ' advantage_mean =%.4f,   loss = %.4f,   train_reward = %.4f'\n",
    "                      % (float(advantage.mean()), loss.item(), float(reward_mean)), detailed_reward)\n",
    "\n",
    "    # Calculate average loss and BLEU scores for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_bleu1 = total_bleu1 / max(num_batches, 1)\n",
    "    avg_bleu2 = total_bleu2 / max(num_batches, 1)\n",
    "    avg_bleu3 = total_bleu3 / max(num_batches, 1)\n",
    "    avg_bleu4 = total_bleu4 / max(num_batches, 1)\n",
    "    \n",
    "    # Log metrics if enabled\n",
    "    if (args.log_bleu or not args.RL_training):\n",
    "        log_training_metrics(epoch, avg_loss, avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, args.log_file)\n",
    "        # print(f\"Epoch {epoch} - Loss: {avg_loss:.6f}, BLEU-1: {avg_bleu1:.4f}, BLEU-2: {avg_bleu2:.4f}, \"\n",
    "        #       f\"BLEU-3: {avg_bleu3:.4f}, BLEU-4: {avg_bleu4:.4f}\")\n",
    "\n",
    "    if epoch%3==0 or epoch==6: #== 0:\n",
    "        torch.save(embeds_shared.state_dict(), os.path.join(args.save_model_path, 'embeds_shared_epoch{}.pth'.format(epoch)))\n",
    "        torch.save(encoder.state_dict(), os.path.join(args.save_model_path, 'encoder_epoch{}.pth'.format(epoch)))\n",
    "        torch.save(decoder.state_dict(), os.path.join(args.save_model_path, 'decoder_epoch{}.pth'.format(epoch)))\n",
    "        print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "\n",
    "def train_TwoAgents(encoder, decoder, device, train_loader, optimizer_encoder, optimizer_decoder, epoch):\n",
    "    global opt_decoder\n",
    "    # if data_parallel: torch.cuda.synchronize()\n",
    "\n",
    "    print('--------------------epoch: %d' % epoch)\n",
    "\n",
    "    for batch_idx, (train_sents, len_batch) in enumerate(train_loader):\n",
    "        # training decoder.\n",
    "        if opt_decoder:\n",
    "            print('training decoder.')\n",
    "            decoder.train()\n",
    "            with torch.no_grad():  # when training decoder, we fix the encoder and make it deterministic, i.e., std=0\n",
    "                encoder.eval()\n",
    "                train_sents = train_sents.to(device)  # with eos\n",
    "                output_float, src_mask = encoder(train_sents, len_batch)\n",
    "                output_float = normlize_layer.apply(output_float)\n",
    "                output = getattr(channel, args.channel_type)(output_float, _snr=args.snr)\n",
    "\n",
    "            output = smaple_n_times(args.multiple_sample, output)\n",
    "            # decoder sample with softmax policy\n",
    "            sample_captions, sample_logprobs, seq_masks = decoder.forward_rl(output, sample_max=False)\n",
    "\n",
    "            fns = list(range(sample_logprobs.size()[0]))\n",
    "            reward_decoder, cider_mean_decoder = get_self_critical_reward_newsc_TXRL(\n",
    "                sample_captions, fns, train_sents, 1, 2, reward_scorer)\n",
    "            loss_decoder = crit('rl', sample_logprobs, seq_masks, reward_decoder) / args.accumulate_grad\n",
    "\n",
    "            loss_decoder.backward()\n",
    "            if (batch_idx + 1) % args.accumulate_grad == 0:\n",
    "                clip_gradient(optimizer_decoder, 0.1)\n",
    "                optimizer_decoder.step()\n",
    "                optimizer_decoder.zero_grad()\n",
    "                opt_decoder = (opt_decoder + 1) % 2  # switch the training flag\n",
    "\n",
    "        else:\n",
    "            print('now we optimizer encoder.')\n",
    "            # now we optimizer encoder.\n",
    "            encoder.train()\n",
    "            train_sents = train_sents.to(device)\n",
    "            output_float_raw, src_mask = encoder(train_sents, len_batch)\n",
    "            output_float = normlize_layer.apply(output_float_raw)\n",
    "            output_float = smaple_n_times(args.multiple_sample, output_float)  # get advantage.\n",
    "            #  when training encoder, we sample with Gaussian policy\n",
    "            output_sampled, logprobs = policy.forward_sample(output_float, std=0.1)\n",
    "            output_sampled = normlize_layer.apply(output_sampled)\n",
    "            output = getattr(channel, args.channel_type)(output_sampled, _snr=args.snr)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # here we fix the decoder and make it deterministic, i.e., `sample_max=True`\n",
    "                decoder.eval()\n",
    "                sample_captions, sample_logprobs, seq_masks = decoder.forward_rl(output, sample_max=True, multiple_sample=1)\n",
    "                fns = list(range(sample_logprobs.size()[0]))\n",
    "                reward_encoder, cider_mean_encoder = get_self_critical_reward_newsc_TXRL(  # all is tensor type.\n",
    "                    sample_captions, fns, train_sents, 1, 2, reward_scorer)\n",
    "            loss_encoder = crit('tx_gaussian_sample', logprobs, reward_encoder) / args.accumulate_grad\n",
    "\n",
    "            loss_encoder.backward()\n",
    "            if (batch_idx + 1) % args.accumulate_grad == 0:\n",
    "                clip_gradient(optimizer_encoder, 0.1)\n",
    "                optimizer_encoder.step()\n",
    "                optimizer_encoder.zero_grad()\n",
    "                opt_decoder = (opt_decoder + 1) % 2\n",
    "\n",
    "        if batch_idx % 100 == 0 and opt_decoder == 0:\n",
    "            # for test only. In this case, both encoder and decoder are deterministic.\n",
    "            # i.e., (encoder: std=0; decoder: argmax)\n",
    "            with torch.no_grad():\n",
    "                output_test = getattr(channel, args.channel_type)(output_float.view(\n",
    "                                                train_sents.shape[0], args.multiple_sample, -1)[:, 0, :],\n",
    "                                                _snr=args.snr)\n",
    "                sample_captions, sample_logprobs, seq_masks = decoder.forward_rl(\n",
    "                                                output_test, sample_max=True, multiple_sample=1)\n",
    "                fns = list(range(sample_logprobs.size()[0]))\n",
    "                _, cider_mean_test = get_self_critical_reward_newsc_TXRL(\n",
    "                    sample_captions, fns, train_sents, 1, 2, reward_scorer)\n",
    "            print('[%4d / %4d]    ' % (batch_idx, epoch), ' cider_mean_decoder =%.4f,   loss_decoder = %.4f,  '\n",
    "                                                          'cider_mean_encoder =%.4f,   loss_encoder = %.4f,   now_cider = %.4f'\n",
    "                  % (float(cider_mean_decoder), loss_decoder.item(), float(cider_mean_encoder),\n",
    "                     loss_encoder.item(), float(cider_mean_test)))\n",
    "\n",
    "    if epoch % 3 == 0:  # == 0:\n",
    "        torch.save(embeds_shared.state_dict(), os.path.join(args.save_model_path, 'embeds_shared_epoch{}.pth'.format(epoch)))\n",
    "        torch.save(encoder.state_dict(),\n",
    "                   os.path.join(args.save_model_path, 'encoder_epoch{}.pth'.format(epoch)))\n",
    "        torch.save(decoder.state_dict(),\n",
    "                   os.path.join(args.save_model_path, 'decoder_epoch{}.pth'.format(epoch)))\n",
    "        print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "\n",
    "\n",
    "# start training\n",
    "print('[*]---------------Start training.')\n",
    "for epoch in range(args.max_epoch):\n",
    "    if epoch >= args.ckpt_resume-1:\n",
    "        if args.SemanticRL_JSCC:\n",
    "            train(encoder, decoder, device, train_data_loader, optimizer, epoch)\n",
    "        else:\n",
    "            train_TwoAgents(encoder, decoder, device, train_data_loader, optimizer_encoder, optimizer_decoder, epoch)\n",
    "    else:\n",
    "        print('skipping epoch ', epoch)\n",
    "    if args.SemanticRL_JSCC:\n",
    "        scheduler.step()\n",
    "    else:  # variant SCSIU\n",
    "        scheduler_encoder.step()\n",
    "        scheduler_decoder.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T06:15:29.819271Z",
     "iopub.status.busy": "2025-08-04T06:15:29.818779Z",
     "iopub.status.idle": "2025-08-04T16:36:11.879149Z",
     "shell.execute_reply": "2025-08-04T16:36:11.878322Z",
     "shell.execute_reply.started": "2025-08-04T06:15:29.819253Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*]---------------args init done\n",
      "args preview:\n",
      "snr: 10\n",
      "multiple_sample: 5\n",
      "scheduled_sampling_start: 18\n",
      "iscomplex: 1\n",
      "channel_type: awgn\n",
      "batch_size: 64\n",
      "channel_dim: 256\n",
      "ckpt_resume: 25\n",
      "lr_milestones: 160\n",
      "max_epoch: 75\n",
      "seeds: 7\n",
      "init_learning_rate: 0.0001\n",
      "save_model_path: ./ckpt_AWGN_CE_Stage2/\n",
      "dataset_path: /kaggle/working/SemanticRL/dataset\n",
      "RL_training: 0\n",
      "reward_type: CIDEr\n",
      "device: cuda:0\n",
      "backbone: LSTM\n",
      "training_config: /kaggle/working/SemanticRL/config/config_AWGN_CE_Stage2.yaml\n",
      "teacher_forcing: 0\n",
      "SemanticRL_JSCC: 1\n",
      "accumulate_grad: 20\n",
      "log_bleu: True\n",
      "log_file: ./results/training_log.txt\n",
      "[*]---------------starting preparing dataset and network. Preparing the dataset may take a few minutes\n",
      "[*]------------vocabulary size is:---- 24064\n",
      "[*]------------sentences size is:---- 709588\n",
      "Model LSTM have 4662272 paramerters in total\n",
      "[*]---------------loaded ckpt at./ckpt_AWGN_CE_Stage2/\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n",
      "[*]---------------network config done.\n",
      "====> get_bleu_scorer end\n",
      "[*]---------------Start training.\n",
      "skipping epoch  0\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "skipping epoch  1\n",
      "skipping epoch  2\n",
      "skipping epoch  3\n",
      "skipping epoch  4\n",
      "skipping epoch  5\n",
      "skipping epoch  6\n",
      "skipping epoch  7\n",
      "skipping epoch  8\n",
      "skipping epoch  9\n",
      "skipping epoch  10\n",
      "skipping epoch  11\n",
      "skipping epoch  12\n",
      "skipping epoch  13\n",
      "skipping epoch  14\n",
      "skipping epoch  15\n",
      "skipping epoch  16\n",
      "skipping epoch  17\n",
      "skipping epoch  18\n",
      "skipping epoch  19\n",
      "skipping epoch  20\n",
      "skipping epoch  21\n",
      "skipping epoch  22\n",
      "skipping epoch  23\n",
      "--------------------epoch: 24\n",
      "[   0 /   24]         loss =  12.199172019958496\n",
      "[ 500 /   24]         loss =  6.092696189880371\n",
      "[1000 /   24]         loss =  4.780591011047363\n",
      "[1500 /   24]         loss =  4.107699394226074\n",
      "[2000 /   24]         loss =  3.637068033218384\n",
      "[2500 /   24]         loss =  3.4062411785125732\n",
      "[3000 /   24]         loss =  3.4031412601470947\n",
      "[3500 /   24]         loss =  3.1005899906158447\n",
      "[4000 /   24]         loss =  3.1492202281951904\n",
      "[4500 /   24]         loss =  2.9593279361724854\n",
      "[5000 /   24]         loss =  2.677370309829712\n",
      "[5500 /   24]         loss =  2.6941936016082764\n",
      "[6000 /   24]         loss =  2.7572667598724365\n",
      "[6500 /   24]         loss =  2.570284366607666\n",
      "[7000 /   24]         loss =  2.5992989540100098\n",
      "[7500 /   24]         loss =  2.4284777641296387\n",
      "[8000 /   24]         loss =  2.3740556240081787\n",
      "[8500 /   24]         loss =  2.301941394805908\n",
      "[9000 /   24]         loss =  2.5016837120056152\n",
      "[9500 /   24]         loss =  2.3656253814697266\n",
      "[10000 /   24]         loss =  2.359250783920288\n",
      "[10500 /   24]         loss =  2.278953790664673\n",
      "[11000 /   24]         loss =  2.331961154937744\n",
      "Epoch 25 model saved!\n",
      "--------------------epoch: 25\n",
      "[   0 /   25]         loss =  2.2471420764923096\n",
      "[ 500 /   25]         loss =  2.370803117752075\n",
      "[1000 /   25]         loss =  2.3349826335906982\n",
      "[1500 /   25]         loss =  2.1744658946990967\n",
      "[2000 /   25]         loss =  2.1888585090637207\n",
      "[2500 /   25]         loss =  2.08524489402771\n",
      "[3000 /   25]         loss =  2.1344730854034424\n",
      "[3500 /   25]         loss =  1.9079575538635254\n",
      "[4000 /   25]         loss =  2.1178572177886963\n",
      "[4500 /   25]         loss =  2.102349281311035\n",
      "[5000 /   25]         loss =  2.0669097900390625\n",
      "[5500 /   25]         loss =  1.9951292276382446\n",
      "[6000 /   25]         loss =  2.1198794841766357\n",
      "[6500 /   25]         loss =  2.1314547061920166\n",
      "[7000 /   25]         loss =  2.308121681213379\n",
      "[7500 /   25]         loss =  2.1288273334503174\n",
      "[8000 /   25]         loss =  2.0797879695892334\n",
      "[8500 /   25]         loss =  2.0875329971313477\n",
      "[9000 /   25]         loss =  1.8862377405166626\n",
      "[9500 /   25]         loss =  2.2516703605651855\n",
      "[10000 /   25]         loss =  2.019178628921509\n",
      "[10500 /   25]         loss =  1.6774194240570068\n",
      "[11000 /   25]         loss =  2.017882823944092\n",
      "--------------------epoch: 26\n",
      "[   0 /   26]         loss =  1.8826653957366943\n",
      "[ 500 /   26]         loss =  2.1572887897491455\n",
      "[1000 /   26]         loss =  1.972293734550476\n",
      "[1500 /   26]         loss =  1.8438001871109009\n",
      "[2000 /   26]         loss =  1.7011924982070923\n",
      "[2500 /   26]         loss =  1.8565590381622314\n",
      "[3000 /   26]         loss =  1.6583737134933472\n",
      "[3500 /   26]         loss =  2.014538049697876\n",
      "[4000 /   26]         loss =  1.9093765020370483\n",
      "[4500 /   26]         loss =  1.7738993167877197\n",
      "[5000 /   26]         loss =  1.8498785495758057\n",
      "[5500 /   26]         loss =  1.7474684715270996\n",
      "[6000 /   26]         loss =  1.8583626747131348\n",
      "[6500 /   26]         loss =  1.9342715740203857\n",
      "[7000 /   26]         loss =  1.9257851839065552\n",
      "[7500 /   26]         loss =  1.659330129623413\n",
      "[8000 /   26]         loss =  1.7578449249267578\n",
      "[8500 /   26]         loss =  1.8465232849121094\n",
      "[9000 /   26]         loss =  1.8759286403656006\n",
      "[9500 /   26]         loss =  1.9562498331069946\n",
      "[10000 /   26]         loss =  1.6979129314422607\n",
      "[10500 /   26]         loss =  1.7401297092437744\n",
      "[11000 /   26]         loss =  1.4832724332809448\n",
      "--------------------epoch: 27\n",
      "[   0 /   27]         loss =  1.7104438543319702\n",
      "[ 500 /   27]         loss =  1.8129112720489502\n",
      "[1000 /   27]         loss =  1.4289915561676025\n",
      "[1500 /   27]         loss =  1.6127151250839233\n",
      "[2000 /   27]         loss =  1.499190330505371\n",
      "[2500 /   27]         loss =  1.7551981210708618\n",
      "[3000 /   27]         loss =  1.808584213256836\n",
      "[3500 /   27]         loss =  1.630057692527771\n",
      "[4000 /   27]         loss =  1.6848914623260498\n",
      "[4500 /   27]         loss =  1.6957975625991821\n",
      "[5000 /   27]         loss =  1.5625665187835693\n",
      "[5500 /   27]         loss =  1.5845065116882324\n",
      "[6000 /   27]         loss =  1.6952396631240845\n",
      "[6500 /   27]         loss =  1.6182068586349487\n",
      "[7000 /   27]         loss =  1.6593542098999023\n",
      "[7500 /   27]         loss =  1.7393510341644287\n",
      "[8000 /   27]         loss =  1.582190990447998\n",
      "[8500 /   27]         loss =  1.655354380607605\n",
      "[9000 /   27]         loss =  1.7188094854354858\n",
      "[9500 /   27]         loss =  1.783605694770813\n",
      "[10000 /   27]         loss =  1.7617286443710327\n",
      "[10500 /   27]         loss =  1.5280605554580688\n",
      "[11000 /   27]         loss =  1.6562631130218506\n",
      "Epoch 28 model saved!\n",
      "--------------------epoch: 28\n",
      "[   0 /   28]         loss =  1.397066593170166\n",
      "[ 500 /   28]         loss =  1.5822170972824097\n",
      "[1000 /   28]         loss =  1.6495208740234375\n",
      "[1500 /   28]         loss =  1.553994059562683\n",
      "[2000 /   28]         loss =  1.565118670463562\n",
      "[2500 /   28]         loss =  1.5556179285049438\n",
      "[3000 /   28]         loss =  1.624921202659607\n",
      "[3500 /   28]         loss =  1.4529211521148682\n",
      "[4000 /   28]         loss =  1.5529990196228027\n",
      "[4500 /   28]         loss =  1.5572330951690674\n",
      "[5000 /   28]         loss =  1.6115792989730835\n",
      "[5500 /   28]         loss =  1.5352238416671753\n",
      "[6000 /   28]         loss =  1.3529750108718872\n",
      "[6500 /   28]         loss =  1.5151729583740234\n",
      "[7000 /   28]         loss =  1.3283307552337646\n",
      "[7500 /   28]         loss =  1.4473180770874023\n",
      "[8000 /   28]         loss =  1.402325987815857\n",
      "[8500 /   28]         loss =  1.4195672273635864\n",
      "[9000 /   28]         loss =  1.5511589050292969\n",
      "[9500 /   28]         loss =  1.4053418636322021\n",
      "[10000 /   28]         loss =  1.3788834810256958\n",
      "[10500 /   28]         loss =  1.4667017459869385\n",
      "[11000 /   28]         loss =  1.6726971864700317\n",
      "--------------------epoch: 29\n",
      "[   0 /   29]         loss =  1.589328646659851\n",
      "[ 500 /   29]         loss =  1.4085599184036255\n",
      "[1000 /   29]         loss =  1.3690001964569092\n",
      "[1500 /   29]         loss =  1.4413985013961792\n",
      "[2000 /   29]         loss =  1.3781319856643677\n",
      "[2500 /   29]         loss =  1.556084156036377\n",
      "[3000 /   29]         loss =  1.4209529161453247\n",
      "[3500 /   29]         loss =  1.487663745880127\n",
      "[4000 /   29]         loss =  1.3682018518447876\n",
      "[4500 /   29]         loss =  1.4892265796661377\n",
      "[5000 /   29]         loss =  1.740458369255066\n",
      "[5500 /   29]         loss =  1.3926160335540771\n",
      "[6000 /   29]         loss =  1.4820393323898315\n",
      "[6500 /   29]         loss =  1.6357535123825073\n",
      "[7000 /   29]         loss =  1.3487218618392944\n",
      "[7500 /   29]         loss =  1.5040611028671265\n",
      "[8000 /   29]         loss =  1.456702470779419\n",
      "[8500 /   29]         loss =  1.3111671209335327\n",
      "[9000 /   29]         loss =  1.30000901222229\n",
      "[9500 /   29]         loss =  1.1732789278030396\n",
      "[10000 /   29]         loss =  1.4306796789169312\n",
      "[10500 /   29]         loss =  1.3687975406646729\n",
      "[11000 /   29]         loss =  1.4438766241073608\n",
      "--------------------epoch: 30\n",
      "[   0 /   30]         loss =  1.4618860483169556\n",
      "[ 500 /   30]         loss =  1.448512315750122\n",
      "[1000 /   30]         loss =  1.512736201286316\n",
      "[1500 /   30]         loss =  1.2165119647979736\n",
      "[2000 /   30]         loss =  1.3336161375045776\n",
      "[2500 /   30]         loss =  1.4703446626663208\n",
      "[3000 /   30]         loss =  1.445968747138977\n",
      "[3500 /   30]         loss =  1.4301272630691528\n",
      "[4000 /   30]         loss =  1.4009640216827393\n",
      "[4500 /   30]         loss =  1.4113123416900635\n",
      "[5000 /   30]         loss =  1.3267494440078735\n",
      "[5500 /   30]         loss =  1.4514424800872803\n",
      "[6000 /   30]         loss =  1.3998146057128906\n",
      "[6500 /   30]         loss =  1.4541019201278687\n",
      "[7000 /   30]         loss =  1.7422035932540894\n",
      "[7500 /   30]         loss =  1.4289186000823975\n",
      "[8000 /   30]         loss =  1.514851450920105\n",
      "[8500 /   30]         loss =  1.50706148147583\n",
      "[9000 /   30]         loss =  1.5346144437789917\n",
      "[9500 /   30]         loss =  1.3702014684677124\n",
      "[10000 /   30]         loss =  1.3981997966766357\n",
      "[10500 /   30]         loss =  1.3342989683151245\n",
      "[11000 /   30]         loss =  1.323617696762085\n",
      "Epoch 31 model saved!\n",
      "--------------------epoch: 31\n",
      "[   0 /   31]         loss =  1.397680640220642\n",
      "[ 500 /   31]         loss =  1.4834270477294922\n",
      "[1000 /   31]         loss =  1.4622946977615356\n",
      "[1500 /   31]         loss =  1.4920704364776611\n",
      "[2000 /   31]         loss =  1.200158715248108\n",
      "[2500 /   31]         loss =  1.4439841508865356\n",
      "[3000 /   31]         loss =  1.2535505294799805\n",
      "[3500 /   31]         loss =  1.235541820526123\n",
      "[4000 /   31]         loss =  1.2997984886169434\n",
      "[4500 /   31]         loss =  1.4403181076049805\n",
      "[5000 /   31]         loss =  1.3358778953552246\n",
      "[5500 /   31]         loss =  1.1089929342269897\n",
      "[6000 /   31]         loss =  1.3102763891220093\n",
      "[6500 /   31]         loss =  1.442923665046692\n",
      "[7000 /   31]         loss =  1.335740566253662\n",
      "[7500 /   31]         loss =  1.4559013843536377\n",
      "[8000 /   31]         loss =  1.224532961845398\n",
      "[8500 /   31]         loss =  1.577998161315918\n",
      "[9000 /   31]         loss =  1.2841802835464478\n",
      "[9500 /   31]         loss =  1.3697270154953003\n",
      "[10000 /   31]         loss =  1.3715033531188965\n",
      "[10500 /   31]         loss =  1.3026572465896606\n",
      "[11000 /   31]         loss =  1.3369587659835815\n",
      "--------------------epoch: 32\n",
      "[   0 /   32]         loss =  1.4690488576889038\n",
      "[ 500 /   32]         loss =  1.4100706577301025\n",
      "[1000 /   32]         loss =  1.3882801532745361\n",
      "[1500 /   32]         loss =  1.4552351236343384\n",
      "[2000 /   32]         loss =  1.7557095289230347\n",
      "[2500 /   32]         loss =  1.4416141510009766\n",
      "[3000 /   32]         loss =  1.4951120615005493\n",
      "[3500 /   32]         loss =  1.3635329008102417\n",
      "[4000 /   32]         loss =  1.5866897106170654\n",
      "[4500 /   32]         loss =  1.4545867443084717\n",
      "[5000 /   32]         loss =  1.5127463340759277\n",
      "[5500 /   32]         loss =  1.6980129480361938\n",
      "[6000 /   32]         loss =  1.366813063621521\n",
      "[6500 /   32]         loss =  1.4904495477676392\n",
      "[7000 /   32]         loss =  1.5268502235412598\n",
      "[7500 /   32]         loss =  1.3577114343643188\n",
      "[8000 /   32]         loss =  1.4835795164108276\n",
      "[8500 /   32]         loss =  1.5331473350524902\n",
      "[9000 /   32]         loss =  1.5393486022949219\n",
      "[9500 /   32]         loss =  1.5725963115692139\n",
      "[10000 /   32]         loss =  1.449414610862732\n",
      "[10500 /   32]         loss =  1.4731017351150513\n",
      "[11000 /   32]         loss =  1.4035677909851074\n",
      "--------------------epoch: 33\n",
      "[   0 /   33]         loss =  1.3452115058898926\n",
      "[ 500 /   33]         loss =  1.4804867506027222\n",
      "[1000 /   33]         loss =  1.4769599437713623\n",
      "[1500 /   33]         loss =  1.4543445110321045\n",
      "[2000 /   33]         loss =  1.3035722970962524\n",
      "[2500 /   33]         loss =  1.2902827262878418\n",
      "[3000 /   33]         loss =  1.4330893754959106\n",
      "[3500 /   33]         loss =  1.5258079767227173\n",
      "[4000 /   33]         loss =  1.3678587675094604\n",
      "[4500 /   33]         loss =  1.6334534883499146\n",
      "[5000 /   33]         loss =  1.360013484954834\n",
      "[5500 /   33]         loss =  1.5195212364196777\n",
      "[6000 /   33]         loss =  1.3679691553115845\n",
      "[6500 /   33]         loss =  1.4848554134368896\n",
      "[7000 /   33]         loss =  1.417982578277588\n",
      "[7500 /   33]         loss =  1.5097075700759888\n",
      "[8000 /   33]         loss =  1.5316715240478516\n",
      "[8500 /   33]         loss =  1.4922683238983154\n",
      "[9000 /   33]         loss =  1.5557466745376587\n",
      "[9500 /   33]         loss =  1.0909115076065063\n",
      "[10000 /   33]         loss =  1.4601918458938599\n",
      "[10500 /   33]         loss =  1.3419814109802246\n",
      "[11000 /   33]         loss =  1.3994513750076294\n",
      "Epoch 34 model saved!\n",
      "--------------------epoch: 34\n",
      "[   0 /   34]         loss =  1.3486870527267456\n",
      "[ 500 /   34]         loss =  1.5111658573150635\n",
      "[1000 /   34]         loss =  1.2414871454238892\n",
      "[1500 /   34]         loss =  1.6595525741577148\n",
      "[2000 /   34]         loss =  1.2690341472625732\n",
      "[2500 /   34]         loss =  1.3634827136993408\n",
      "[3000 /   34]         loss =  1.3992911577224731\n",
      "[3500 /   34]         loss =  1.3756343126296997\n",
      "[4000 /   34]         loss =  1.2867565155029297\n",
      "[4500 /   34]         loss =  1.2030378580093384\n",
      "[5000 /   34]         loss =  1.1980984210968018\n",
      "[5500 /   34]         loss =  1.4224002361297607\n",
      "[6000 /   34]         loss =  1.3200653791427612\n",
      "[6500 /   34]         loss =  1.4952514171600342\n",
      "[7000 /   34]         loss =  1.3065762519836426\n",
      "[7500 /   34]         loss =  1.3377885818481445\n",
      "[8000 /   34]         loss =  1.6640284061431885\n",
      "[8500 /   34]         loss =  1.2971205711364746\n",
      "[9000 /   34]         loss =  1.5604150295257568\n",
      "[9500 /   34]         loss =  1.6256418228149414\n",
      "[10000 /   34]         loss =  1.1254891157150269\n",
      "[10500 /   34]         loss =  1.36326265335083\n",
      "[11000 /   34]         loss =  1.3823198080062866\n",
      "--------------------epoch: 35\n",
      "[   0 /   35]         loss =  1.5038865804672241\n",
      "[ 500 /   35]         loss =  1.403774380683899\n",
      "[1000 /   35]         loss =  1.3626034259796143\n",
      "[1500 /   35]         loss =  1.2340590953826904\n",
      "[2000 /   35]         loss =  1.423018217086792\n",
      "[2500 /   35]         loss =  1.326554536819458\n",
      "[3000 /   35]         loss =  1.3930169343948364\n",
      "[3500 /   35]         loss =  1.3886853456497192\n",
      "[4000 /   35]         loss =  1.4156936407089233\n",
      "[4500 /   35]         loss =  1.2064820528030396\n",
      "[5000 /   35]         loss =  1.4906773567199707\n",
      "[5500 /   35]         loss =  1.3733515739440918\n",
      "[6000 /   35]         loss =  1.3070663213729858\n",
      "[6500 /   35]         loss =  1.4353628158569336\n",
      "[7000 /   35]         loss =  1.3634625673294067\n",
      "[7500 /   35]         loss =  1.412583827972412\n",
      "[8000 /   35]         loss =  1.2040607929229736\n",
      "[8500 /   35]         loss =  1.4442640542984009\n",
      "[9000 /   35]         loss =  1.477753758430481\n",
      "[9500 /   35]         loss =  1.637964129447937\n",
      "[10000 /   35]         loss =  1.4930163621902466\n",
      "[10500 /   35]         loss =  1.3610825538635254\n",
      "[11000 /   35]         loss =  1.5023325681686401\n",
      "--------------------epoch: 36\n",
      "[   0 /   36]         loss =  1.3382883071899414\n",
      "[ 500 /   36]         loss =  1.290328860282898\n",
      "[1000 /   36]         loss =  1.3226954936981201\n",
      "[1500 /   36]         loss =  1.1828852891921997\n",
      "[2000 /   36]         loss =  1.4302703142166138\n",
      "[2500 /   36]         loss =  1.1342988014221191\n",
      "[3000 /   36]         loss =  1.4653881788253784\n",
      "[3500 /   36]         loss =  1.4343929290771484\n",
      "[4000 /   36]         loss =  1.36679208278656\n",
      "[4500 /   36]         loss =  1.477236032485962\n",
      "[5000 /   36]         loss =  1.258705735206604\n",
      "[5500 /   36]         loss =  1.3886774778366089\n",
      "[6000 /   36]         loss =  1.2599443197250366\n",
      "[6500 /   36]         loss =  1.1765257120132446\n",
      "[7000 /   36]         loss =  1.3220247030258179\n",
      "[7500 /   36]         loss =  1.2436771392822266\n",
      "[8000 /   36]         loss =  1.3636751174926758\n",
      "[8500 /   36]         loss =  1.5698391199111938\n",
      "[9000 /   36]         loss =  1.5217797756195068\n",
      "[9500 /   36]         loss =  1.1383947134017944\n",
      "[10000 /   36]         loss =  1.2723636627197266\n",
      "[10500 /   36]         loss =  1.3488101959228516\n",
      "[11000 /   36]         loss =  1.4625163078308105\n",
      "Epoch 37 model saved!\n",
      "--------------------epoch: 37\n",
      "[   0 /   37]         loss =  1.323697805404663\n",
      "[ 500 /   37]         loss =  1.080594778060913\n",
      "[1000 /   37]         loss =  1.2031588554382324\n",
      "[1500 /   37]         loss =  1.2771599292755127\n",
      "[2000 /   37]         loss =  1.2335423231124878\n",
      "[2500 /   37]         loss =  1.4269136190414429\n",
      "[3000 /   37]         loss =  1.1611957550048828\n",
      "[3500 /   37]         loss =  1.2641985416412354\n",
      "[4000 /   37]         loss =  1.1608295440673828\n",
      "[4500 /   37]         loss =  1.261704921722412\n",
      "[5000 /   37]         loss =  1.3011730909347534\n",
      "[5500 /   37]         loss =  1.421570062637329\n",
      "[6000 /   37]         loss =  1.2413822412490845\n",
      "[6500 /   37]         loss =  1.5128915309906006\n",
      "[7000 /   37]         loss =  1.1494905948638916\n",
      "[7500 /   37]         loss =  1.1921300888061523\n",
      "[8000 /   37]         loss =  1.164237380027771\n",
      "[8500 /   37]         loss =  1.344592571258545\n",
      "[9000 /   37]         loss =  1.4117505550384521\n",
      "[9500 /   37]         loss =  1.2246925830841064\n",
      "[10000 /   37]         loss =  1.322508692741394\n",
      "[10500 /   37]         loss =  1.2549245357513428\n",
      "[11000 /   37]         loss =  1.4200174808502197\n",
      "--------------------epoch: 38\n",
      "[   0 /   38]         loss =  1.240724802017212\n",
      "[ 500 /   38]         loss =  1.4453476667404175\n",
      "[1000 /   38]         loss =  1.365910291671753\n",
      "[1500 /   38]         loss =  1.315466284751892\n",
      "[2000 /   38]         loss =  1.3345550298690796\n",
      "[2500 /   38]         loss =  1.2253419160842896\n",
      "[3000 /   38]         loss =  1.3552004098892212\n",
      "[3500 /   38]         loss =  1.3240892887115479\n",
      "[4000 /   38]         loss =  1.1020567417144775\n",
      "[4500 /   38]         loss =  1.283117413520813\n",
      "[5000 /   38]         loss =  1.186008334159851\n",
      "[5500 /   38]         loss =  1.2827091217041016\n",
      "[6000 /   38]         loss =  1.288409948348999\n",
      "[6500 /   38]         loss =  1.0614324808120728\n",
      "[7000 /   38]         loss =  1.3003156185150146\n",
      "[7500 /   38]         loss =  1.42445969581604\n",
      "[8000 /   38]         loss =  1.1322436332702637\n",
      "[8500 /   38]         loss =  1.2747118473052979\n",
      "[9000 /   38]         loss =  1.3187813758850098\n",
      "[9500 /   38]         loss =  1.5010048151016235\n",
      "[10000 /   38]         loss =  1.2777971029281616\n",
      "[10500 /   38]         loss =  0.8967406153678894\n",
      "[11000 /   38]         loss =  1.3012772798538208\n",
      "--------------------epoch: 39\n",
      "[   0 /   39]         loss =  1.4036945104599\n",
      "[ 500 /   39]         loss =  1.4509398937225342\n",
      "[1000 /   39]         loss =  1.3473845720291138\n",
      "[1500 /   39]         loss =  1.4904955625534058\n",
      "[2000 /   39]         loss =  1.2857484817504883\n",
      "[2500 /   39]         loss =  1.3978554010391235\n",
      "[3000 /   39]         loss =  1.280983805656433\n",
      "[3500 /   39]         loss =  1.0598101615905762\n",
      "[4000 /   39]         loss =  1.277382731437683\n",
      "[4500 /   39]         loss =  1.4033375978469849\n",
      "[5000 /   39]         loss =  1.1904590129852295\n",
      "[5500 /   39]         loss =  1.223175287246704\n",
      "[6000 /   39]         loss =  1.4192969799041748\n",
      "[6500 /   39]         loss =  1.376710057258606\n",
      "[7000 /   39]         loss =  1.2491490840911865\n",
      "[7500 /   39]         loss =  1.4102840423583984\n",
      "[8000 /   39]         loss =  1.4539413452148438\n",
      "[8500 /   39]         loss =  1.3946222066879272\n",
      "[9000 /   39]         loss =  1.3576719760894775\n",
      "[9500 /   39]         loss =  1.4230906963348389\n",
      "[10000 /   39]         loss =  1.3605364561080933\n",
      "[10500 /   39]         loss =  1.3027946949005127\n",
      "[11000 /   39]         loss =  1.3636373281478882\n",
      "Epoch 40 model saved!\n",
      "--------------------epoch: 40\n",
      "[   0 /   40]         loss =  1.238795280456543\n",
      "[ 500 /   40]         loss =  1.3168200254440308\n",
      "[1000 /   40]         loss =  1.3824139833450317\n",
      "[1500 /   40]         loss =  1.401739239692688\n",
      "[2000 /   40]         loss =  1.3659982681274414\n",
      "[2500 /   40]         loss =  1.3591058254241943\n",
      "[3000 /   40]         loss =  1.3714673519134521\n",
      "[3500 /   40]         loss =  1.3607933521270752\n",
      "[4000 /   40]         loss =  1.38557767868042\n",
      "[4500 /   40]         loss =  1.4214352369308472\n",
      "[5000 /   40]         loss =  1.1823385953903198\n",
      "[5500 /   40]         loss =  1.325039267539978\n",
      "[6000 /   40]         loss =  1.0977909564971924\n",
      "[6500 /   40]         loss =  1.3410590887069702\n",
      "[7000 /   40]         loss =  1.372363567352295\n",
      "[7500 /   40]         loss =  1.472206711769104\n",
      "[8000 /   40]         loss =  1.3202866315841675\n",
      "[8500 /   40]         loss =  1.283619999885559\n",
      "[9000 /   40]         loss =  1.5408443212509155\n",
      "[9500 /   40]         loss =  1.4897334575653076\n",
      "[10000 /   40]         loss =  1.4970579147338867\n",
      "[10500 /   40]         loss =  1.3191648721694946\n",
      "[11000 /   40]         loss =  1.2666196823120117\n",
      "--------------------epoch: 41\n",
      "[   0 /   41]         loss =  1.1998133659362793\n",
      "[ 500 /   41]         loss =  1.2541389465332031\n",
      "[1000 /   41]         loss =  1.3333909511566162\n",
      "[1500 /   41]         loss =  1.2838271856307983\n",
      "[2000 /   41]         loss =  1.3642194271087646\n",
      "[2500 /   41]         loss =  1.38245689868927\n",
      "[3000 /   41]         loss =  1.2768265008926392\n",
      "[3500 /   41]         loss =  1.4119118452072144\n",
      "[4000 /   41]         loss =  1.3704540729522705\n",
      "[4500 /   41]         loss =  1.5185753107070923\n",
      "[5000 /   41]         loss =  1.3378509283065796\n",
      "[5500 /   41]         loss =  1.6279911994934082\n",
      "[6000 /   41]         loss =  1.3773070573806763\n",
      "[6500 /   41]         loss =  1.5694714784622192\n",
      "[7000 /   41]         loss =  1.281044363975525\n",
      "[7500 /   41]         loss =  1.2980698347091675\n",
      "[8000 /   41]         loss =  1.4374266862869263\n",
      "[8500 /   41]         loss =  1.4668792486190796\n",
      "[9000 /   41]         loss =  1.3767940998077393\n",
      "[9500 /   41]         loss =  1.2952933311462402\n",
      "[10000 /   41]         loss =  1.163709044456482\n",
      "[10500 /   41]         loss =  1.4409949779510498\n",
      "[11000 /   41]         loss =  1.452599287033081\n",
      "--------------------epoch: 42\n",
      "[   0 /   42]         loss =  1.3680500984191895\n",
      "[ 500 /   42]         loss =  1.2010762691497803\n",
      "[1000 /   42]         loss =  1.3139013051986694\n",
      "[1500 /   42]         loss =  1.3520153760910034\n",
      "[2000 /   42]         loss =  1.1862294673919678\n",
      "[2500 /   42]         loss =  1.3422194719314575\n",
      "[3000 /   42]         loss =  1.3168102502822876\n",
      "[3500 /   42]         loss =  1.2583502531051636\n",
      "[4000 /   42]         loss =  1.3433501720428467\n",
      "[4500 /   42]         loss =  1.4381402730941772\n",
      "[5000 /   42]         loss =  1.4984115362167358\n",
      "[5500 /   42]         loss =  1.195054531097412\n",
      "[6000 /   42]         loss =  1.2672640085220337\n",
      "[6500 /   42]         loss =  1.2823078632354736\n",
      "[7000 /   42]         loss =  1.3535571098327637\n",
      "[7500 /   42]         loss =  1.2356983423233032\n",
      "[8000 /   42]         loss =  1.1889152526855469\n",
      "[8500 /   42]         loss =  1.4265323877334595\n",
      "[9000 /   42]         loss =  1.2888073921203613\n",
      "[9500 /   42]         loss =  1.2592147588729858\n",
      "[10000 /   42]         loss =  1.4803166389465332\n",
      "[10500 /   42]         loss =  1.3624461889266968\n",
      "[11000 /   42]         loss =  1.3855153322219849\n",
      "Epoch 43 model saved!\n",
      "--------------------epoch: 43\n",
      "[   0 /   43]         loss =  1.1857481002807617\n",
      "[ 500 /   43]         loss =  1.2891936302185059\n",
      "[1000 /   43]         loss =  1.54905366897583\n",
      "[1500 /   43]         loss =  1.274707555770874\n",
      "[2000 /   43]         loss =  1.5151337385177612\n",
      "[2500 /   43]         loss =  1.3136928081512451\n",
      "[3000 /   43]         loss =  1.3349443674087524\n",
      "[3500 /   43]         loss =  1.2674198150634766\n",
      "[4000 /   43]         loss =  1.527146816253662\n",
      "[4500 /   43]         loss =  1.5047205686569214\n",
      "[5000 /   43]         loss =  1.4683347940444946\n",
      "[5500 /   43]         loss =  1.1026369333267212\n",
      "[6000 /   43]         loss =  1.4391627311706543\n",
      "[6500 /   43]         loss =  1.25237238407135\n",
      "[7000 /   43]         loss =  1.4491875171661377\n",
      "[7500 /   43]         loss =  1.4204869270324707\n",
      "[8000 /   43]         loss =  1.4788559675216675\n",
      "[8500 /   43]         loss =  1.4627057313919067\n",
      "[9000 /   43]         loss =  1.303998351097107\n",
      "[9500 /   43]         loss =  1.3824917078018188\n",
      "[10000 /   43]         loss =  1.4288274049758911\n",
      "[10500 /   43]         loss =  1.253512978553772\n",
      "[11000 /   43]         loss =  1.3738709688186646\n",
      "--------------------epoch: 44\n",
      "[   0 /   44]         loss =  1.1758475303649902\n",
      "[ 500 /   44]         loss =  1.349418044090271\n",
      "[1000 /   44]         loss =  1.3833154439926147\n",
      "[1500 /   44]         loss =  1.1726739406585693\n",
      "[2000 /   44]         loss =  1.5873682498931885\n",
      "[2500 /   44]         loss =  1.2564657926559448\n",
      "[3000 /   44]         loss =  1.3442484140396118\n",
      "[3500 /   44]         loss =  1.1721996068954468\n",
      "[4000 /   44]         loss =  1.2294752597808838\n",
      "[4500 /   44]         loss =  1.0995148420333862\n",
      "[5000 /   44]         loss =  1.308617115020752\n",
      "[5500 /   44]         loss =  1.0697022676467896\n",
      "[6000 /   44]         loss =  1.1400784254074097\n",
      "[6500 /   44]         loss =  1.3136922121047974\n",
      "[7000 /   44]         loss =  1.134842038154602\n",
      "[7500 /   44]         loss =  1.216563105583191\n",
      "[8000 /   44]         loss =  1.2484666109085083\n",
      "[8500 /   44]         loss =  1.179670810699463\n",
      "[9000 /   44]         loss =  1.3833370208740234\n",
      "[9500 /   44]         loss =  1.4286507368087769\n",
      "[10000 /   44]         loss =  1.2842402458190918\n",
      "[10500 /   44]         loss =  1.4067806005477905\n",
      "[11000 /   44]         loss =  1.2817981243133545\n",
      "--------------------epoch: 45\n",
      "[   0 /   45]         loss =  1.5992769002914429\n",
      "[ 500 /   45]         loss =  1.2261207103729248\n",
      "[1000 /   45]         loss =  1.325333833694458\n",
      "[1500 /   45]         loss =  1.4648065567016602\n",
      "[2000 /   45]         loss =  1.328843355178833\n",
      "[2500 /   45]         loss =  1.4530562162399292\n",
      "[3000 /   45]         loss =  1.3844637870788574\n",
      "[3500 /   45]         loss =  1.3610254526138306\n",
      "[4000 /   45]         loss =  1.3555330038070679\n",
      "[4500 /   45]         loss =  1.3448032140731812\n",
      "[5000 /   45]         loss =  1.1785314083099365\n",
      "[5500 /   45]         loss =  1.1009567975997925\n",
      "[6000 /   45]         loss =  1.3738276958465576\n",
      "[6500 /   45]         loss =  1.3637808561325073\n",
      "[7000 /   45]         loss =  1.3602750301361084\n",
      "[7500 /   45]         loss =  1.2281373739242554\n",
      "[8000 /   45]         loss =  1.121159553527832\n",
      "[8500 /   45]         loss =  1.4174197912216187\n",
      "[9000 /   45]         loss =  1.3960238695144653\n",
      "[9500 /   45]         loss =  1.2701712846755981\n",
      "[10000 /   45]         loss =  1.3216601610183716\n",
      "[10500 /   45]         loss =  1.3456833362579346\n",
      "[11000 /   45]         loss =  1.4377427101135254\n",
      "Epoch 46 model saved!\n",
      "--------------------epoch: 46\n",
      "[   0 /   46]         loss =  1.3306729793548584\n",
      "[ 500 /   46]         loss =  1.4340962171554565\n",
      "[1000 /   46]         loss =  1.3963394165039062\n",
      "[1500 /   46]         loss =  1.5372793674468994\n",
      "[2000 /   46]         loss =  1.3585431575775146\n",
      "[2500 /   46]         loss =  1.4929004907608032\n",
      "[3000 /   46]         loss =  1.3701248168945312\n",
      "[3500 /   46]         loss =  1.5890157222747803\n",
      "[4000 /   46]         loss =  1.384133219718933\n",
      "[4500 /   46]         loss =  1.2938934564590454\n",
      "[5000 /   46]         loss =  1.2659649848937988\n",
      "[5500 /   46]         loss =  1.4558666944503784\n",
      "[6000 /   46]         loss =  1.3361722230911255\n",
      "[6500 /   46]         loss =  1.420372486114502\n",
      "[7000 /   46]         loss =  1.3047610521316528\n",
      "[7500 /   46]         loss =  1.2257412672042847\n",
      "[8000 /   46]         loss =  1.459033489227295\n",
      "[8500 /   46]         loss =  1.3203896284103394\n",
      "[9000 /   46]         loss =  1.4801313877105713\n",
      "[9500 /   46]         loss =  1.4596909284591675\n",
      "[10000 /   46]         loss =  1.402363657951355\n",
      "[10500 /   46]         loss =  1.4538828134536743\n",
      "[11000 /   46]         loss =  1.3759022951126099\n",
      "--------------------epoch: 47\n",
      "[   0 /   47]         loss =  1.400524377822876\n",
      "[ 500 /   47]         loss =  1.4913519620895386\n",
      "[1000 /   47]         loss =  1.3297117948532104\n",
      "[1500 /   47]         loss =  1.5436079502105713\n",
      "[2000 /   47]         loss =  1.3611708879470825\n",
      "[2500 /   47]         loss =  1.5316417217254639\n",
      "[3000 /   47]         loss =  1.7796967029571533\n",
      "[3500 /   47]         loss =  1.5107011795043945\n",
      "[4000 /   47]         loss =  1.4224029779434204\n",
      "[4500 /   47]         loss =  1.3431425094604492\n",
      "[5000 /   47]         loss =  1.175986886024475\n",
      "[5500 /   47]         loss =  1.3766542673110962\n",
      "[6000 /   47]         loss =  1.5003609657287598\n",
      "[6500 /   47]         loss =  1.4623111486434937\n",
      "[7000 /   47]         loss =  1.4194655418395996\n",
      "[7500 /   47]         loss =  1.498183250427246\n",
      "[8000 /   47]         loss =  1.2994648218154907\n",
      "[8500 /   47]         loss =  1.3681755065917969\n",
      "[9000 /   47]         loss =  1.3191726207733154\n",
      "[9500 /   47]         loss =  1.5249626636505127\n",
      "[10000 /   47]         loss =  1.4560260772705078\n",
      "[10500 /   47]         loss =  1.2328989505767822\n",
      "[11000 /   47]         loss =  1.4290509223937988\n",
      "--------------------epoch: 48\n",
      "[   0 /   48]         loss =  1.2767328023910522\n",
      "[ 500 /   48]         loss =  1.3626110553741455\n",
      "[1000 /   48]         loss =  1.22639799118042\n",
      "[1500 /   48]         loss =  1.3755089044570923\n",
      "[2000 /   48]         loss =  1.407519817352295\n",
      "[2500 /   48]         loss =  1.417554259300232\n",
      "[3000 /   48]         loss =  1.379703164100647\n",
      "[3500 /   48]         loss =  1.5200932025909424\n",
      "[4000 /   48]         loss =  1.4636924266815186\n",
      "[4500 /   48]         loss =  1.3255916833877563\n",
      "[5000 /   48]         loss =  1.4559463262557983\n",
      "[5500 /   48]         loss =  1.4852532148361206\n",
      "[6000 /   48]         loss =  1.3991273641586304\n",
      "[6500 /   48]         loss =  1.2932273149490356\n",
      "[7000 /   48]         loss =  1.3712081909179688\n",
      "[7500 /   48]         loss =  1.3564590215682983\n",
      "[8000 /   48]         loss =  1.3670005798339844\n",
      "[8500 /   48]         loss =  1.4482911825180054\n",
      "[9000 /   48]         loss =  1.521939754486084\n",
      "[9500 /   48]         loss =  1.4248875379562378\n",
      "[10000 /   48]         loss =  1.1249933242797852\n",
      "[10500 /   48]         loss =  1.4416863918304443\n",
      "[11000 /   48]         loss =  1.35749089717865\n",
      "Epoch 49 model saved!\n",
      "--------------------epoch: 49\n",
      "[   0 /   49]         loss =  1.3693820238113403\n",
      "[ 500 /   49]         loss =  1.4917289018630981\n",
      "[1000 /   49]         loss =  1.2828562259674072\n",
      "[1500 /   49]         loss =  1.5547981262207031\n",
      "[2000 /   49]         loss =  1.523536205291748\n",
      "[2500 /   49]         loss =  1.4705406427383423\n",
      "[3000 /   49]         loss =  1.2823853492736816\n",
      "[3500 /   49]         loss =  1.1277899742126465\n",
      "[4000 /   49]         loss =  1.324812889099121\n",
      "[4500 /   49]         loss =  1.5201534032821655\n",
      "[5000 /   49]         loss =  1.327561855316162\n",
      "[5500 /   49]         loss =  1.4304330348968506\n",
      "[6000 /   49]         loss =  1.451329231262207\n",
      "[6500 /   49]         loss =  1.367944359779358\n",
      "[7000 /   49]         loss =  1.6208707094192505\n",
      "[7500 /   49]         loss =  1.1972779035568237\n",
      "[8000 /   49]         loss =  1.3005746603012085\n",
      "[8500 /   49]         loss =  1.4796754121780396\n",
      "[9000 /   49]         loss =  1.4296925067901611\n",
      "[9500 /   49]         loss =  1.5002235174179077\n",
      "[10000 /   49]         loss =  1.1343162059783936\n",
      "[10500 /   49]         loss =  1.4614042043685913\n",
      "[11000 /   49]         loss =  1.4742366075515747\n",
      "--------------------epoch: 50\n",
      "[   0 /   50]         loss =  1.331421136856079\n",
      "[ 500 /   50]         loss =  1.1188031435012817\n",
      "[1000 /   50]         loss =  1.5847702026367188\n",
      "[1500 /   50]         loss =  1.4773494005203247\n",
      "[2000 /   50]         loss =  1.3603310585021973\n",
      "[2500 /   50]         loss =  1.4970041513442993\n",
      "[3000 /   50]         loss =  1.2266603708267212\n",
      "[3500 /   50]         loss =  1.5052608251571655\n",
      "[4000 /   50]         loss =  1.3281471729278564\n",
      "[4500 /   50]         loss =  1.2285425662994385\n",
      "[5000 /   50]         loss =  1.1988939046859741\n",
      "[5500 /   50]         loss =  1.3929293155670166\n",
      "[6000 /   50]         loss =  1.2002259492874146\n",
      "[6500 /   50]         loss =  1.1986312866210938\n",
      "[7000 /   50]         loss =  1.2530994415283203\n",
      "[7500 /   50]         loss =  1.2489429712295532\n",
      "[8000 /   50]         loss =  1.113629698753357\n",
      "[8500 /   50]         loss =  1.340376615524292\n",
      "[9000 /   50]         loss =  1.418131709098816\n",
      "[9500 /   50]         loss =  1.5306580066680908\n",
      "[10000 /   50]         loss =  1.3524996042251587\n",
      "[10500 /   50]         loss =  1.4403635263442993\n",
      "[11000 /   50]         loss =  1.6559854745864868\n",
      "--------------------epoch: 51\n",
      "[   0 /   51]         loss =  1.5125806331634521\n",
      "[ 500 /   51]         loss =  1.3737401962280273\n",
      "[1000 /   51]         loss =  1.3704849481582642\n",
      "[1500 /   51]         loss =  1.5620293617248535\n",
      "[2000 /   51]         loss =  1.5612224340438843\n",
      "[2500 /   51]         loss =  1.4226611852645874\n",
      "[3000 /   51]         loss =  1.3587028980255127\n",
      "[3500 /   51]         loss =  1.5631409883499146\n",
      "[4000 /   51]         loss =  1.3533884286880493\n",
      "[4500 /   51]         loss =  1.2998241186141968\n",
      "[5000 /   51]         loss =  1.257275938987732\n",
      "[5500 /   51]         loss =  1.4471944570541382\n",
      "[6000 /   51]         loss =  1.402292013168335\n",
      "[6500 /   51]         loss =  1.355607271194458\n",
      "[7000 /   51]         loss =  1.5500764846801758\n",
      "[7500 /   51]         loss =  1.5267163515090942\n",
      "[8000 /   51]         loss =  1.3195154666900635\n",
      "[8500 /   51]         loss =  1.2922565937042236\n",
      "[9000 /   51]         loss =  1.367261290550232\n",
      "[9500 /   51]         loss =  1.539232611656189\n",
      "[10000 /   51]         loss =  1.2729796171188354\n",
      "[10500 /   51]         loss =  1.2521369457244873\n",
      "[11000 /   51]         loss =  1.553378701210022\n",
      "Epoch 52 model saved!\n",
      "--------------------epoch: 52\n",
      "[   0 /   52]         loss =  1.3055673837661743\n",
      "[ 500 /   52]         loss =  1.3154072761535645\n",
      "[1000 /   52]         loss =  1.3472719192504883\n",
      "[1500 /   52]         loss =  1.3105374574661255\n",
      "[2000 /   52]         loss =  1.333509922027588\n",
      "[2500 /   52]         loss =  1.2585901021957397\n",
      "[3000 /   52]         loss =  1.4638702869415283\n",
      "[3500 /   52]         loss =  1.2715799808502197\n",
      "[4000 /   52]         loss =  1.0706981420516968\n",
      "[4500 /   52]         loss =  1.2592816352844238\n",
      "[5000 /   52]         loss =  1.2505137920379639\n",
      "[5500 /   52]         loss =  1.3625928163528442\n",
      "[6000 /   52]         loss =  1.4896962642669678\n",
      "[6500 /   52]         loss =  1.2263368368148804\n",
      "[7000 /   52]         loss =  1.3791375160217285\n",
      "[7500 /   52]         loss =  1.3054213523864746\n",
      "[8000 /   52]         loss =  1.3313864469528198\n",
      "[8500 /   52]         loss =  1.4574384689331055\n",
      "[9000 /   52]         loss =  1.313493013381958\n",
      "[9500 /   52]         loss =  1.248802661895752\n",
      "[10000 /   52]         loss =  1.3342047929763794\n",
      "[10500 /   52]         loss =  1.375973105430603\n",
      "[11000 /   52]         loss =  1.4216375350952148\n",
      "--------------------epoch: 53\n",
      "[   0 /   53]         loss =  1.4730387926101685\n",
      "[ 500 /   53]         loss =  1.518349528312683\n",
      "[1000 /   53]         loss =  1.662380337715149\n",
      "[1500 /   53]         loss =  1.3350015878677368\n",
      "[2000 /   53]         loss =  1.4264849424362183\n",
      "[2500 /   53]         loss =  1.4447834491729736\n",
      "[3000 /   53]         loss =  1.3960516452789307\n",
      "[3500 /   53]         loss =  1.3528666496276855\n",
      "[4000 /   53]         loss =  1.2894573211669922\n",
      "[4500 /   53]         loss =  1.5060147047042847\n",
      "[5000 /   53]         loss =  1.2948089838027954\n",
      "[5500 /   53]         loss =  1.3408414125442505\n",
      "[6000 /   53]         loss =  1.5546754598617554\n",
      "[6500 /   53]         loss =  1.337352991104126\n",
      "[7000 /   53]         loss =  1.3685532808303833\n",
      "[7500 /   53]         loss =  1.522650957107544\n",
      "[8000 /   53]         loss =  1.4347774982452393\n",
      "[8500 /   53]         loss =  1.5231037139892578\n",
      "[9000 /   53]         loss =  1.4142231941223145\n",
      "[9500 /   53]         loss =  1.3717271089553833\n",
      "[10000 /   53]         loss =  1.490424394607544\n",
      "[10500 /   53]         loss =  1.4441063404083252\n",
      "[11000 /   53]         loss =  1.4684396982192993\n",
      "--------------------epoch: 54\n",
      "[   0 /   54]         loss =  1.2053403854370117\n",
      "[ 500 /   54]         loss =  1.4489338397979736\n",
      "[1000 /   54]         loss =  1.5153745412826538\n",
      "[1500 /   54]         loss =  1.457055926322937\n",
      "[2000 /   54]         loss =  1.202083945274353\n",
      "[2500 /   54]         loss =  1.3261075019836426\n",
      "[3000 /   54]         loss =  1.4656981229782104\n",
      "[3500 /   54]         loss =  1.368996262550354\n",
      "[4000 /   54]         loss =  1.5017642974853516\n",
      "[4500 /   54]         loss =  1.2362353801727295\n",
      "[5000 /   54]         loss =  1.565367579460144\n",
      "[5500 /   54]         loss =  1.4205762147903442\n",
      "[6000 /   54]         loss =  1.5220346450805664\n",
      "[6500 /   54]         loss =  1.2125213146209717\n",
      "[7000 /   54]         loss =  1.4690306186676025\n",
      "[7500 /   54]         loss =  1.5390934944152832\n",
      "[8000 /   54]         loss =  1.2844371795654297\n",
      "[8500 /   54]         loss =  1.3662112951278687\n",
      "[9000 /   54]         loss =  1.5305941104888916\n",
      "[9500 /   54]         loss =  1.3716487884521484\n",
      "[10000 /   54]         loss =  1.3632487058639526\n",
      "[10500 /   54]         loss =  1.3402667045593262\n",
      "[11000 /   54]         loss =  1.2856782674789429\n",
      "Epoch 55 model saved!\n",
      "--------------------epoch: 55\n",
      "[   0 /   55]         loss =  1.2158570289611816\n",
      "[ 500 /   55]         loss =  1.3272303342819214\n",
      "[1000 /   55]         loss =  1.4195722341537476\n",
      "[1500 /   55]         loss =  1.467855453491211\n",
      "[2000 /   55]         loss =  1.472686767578125\n",
      "[2500 /   55]         loss =  1.415652871131897\n",
      "[3000 /   55]         loss =  1.3409992456436157\n",
      "[3500 /   55]         loss =  1.2860918045043945\n",
      "[4000 /   55]         loss =  1.3963632583618164\n",
      "[4500 /   55]         loss =  1.4771660566329956\n",
      "[5000 /   55]         loss =  1.3735471963882446\n",
      "[5500 /   55]         loss =  1.444189429283142\n",
      "[6000 /   55]         loss =  1.070217490196228\n",
      "[6500 /   55]         loss =  1.1410996913909912\n",
      "[7000 /   55]         loss =  1.4629085063934326\n",
      "[7500 /   55]         loss =  1.514859676361084\n",
      "[8000 /   55]         loss =  1.4702723026275635\n",
      "[8500 /   55]         loss =  1.3464837074279785\n",
      "[9000 /   55]         loss =  1.2809226512908936\n",
      "[9500 /   55]         loss =  1.360420823097229\n",
      "[10000 /   55]         loss =  1.6429917812347412\n",
      "[10500 /   55]         loss =  1.2796999216079712\n",
      "[11000 /   55]         loss =  1.3797898292541504\n",
      "--------------------epoch: 56\n",
      "[   0 /   56]         loss =  1.4346109628677368\n",
      "[ 500 /   56]         loss =  1.1662800312042236\n",
      "[1000 /   56]         loss =  1.3489458560943604\n",
      "[1500 /   56]         loss =  1.5890417098999023\n",
      "[2000 /   56]         loss =  1.2921128273010254\n",
      "[2500 /   56]         loss =  1.3596047163009644\n",
      "[3000 /   56]         loss =  1.3537561893463135\n",
      "[3500 /   56]         loss =  1.8401365280151367\n",
      "[4000 /   56]         loss =  1.3962258100509644\n",
      "[4500 /   56]         loss =  1.3179081678390503\n",
      "[5000 /   56]         loss =  1.3200665712356567\n",
      "[5500 /   56]         loss =  1.4559121131896973\n",
      "[6000 /   56]         loss =  1.3360806703567505\n",
      "[6500 /   56]         loss =  1.2629814147949219\n",
      "[7000 /   56]         loss =  1.257393479347229\n",
      "[7500 /   56]         loss =  1.5602539777755737\n",
      "[8000 /   56]         loss =  1.420644760131836\n",
      "[8500 /   56]         loss =  1.380388855934143\n",
      "[9000 /   56]         loss =  1.401143193244934\n",
      "[9500 /   56]         loss =  1.5298974514007568\n",
      "[10000 /   56]         loss =  1.1737133264541626\n",
      "[10500 /   56]         loss =  1.3206512928009033\n",
      "[11000 /   56]         loss =  1.446656584739685\n",
      "--------------------epoch: 57\n",
      "[   0 /   57]         loss =  1.393394112586975\n",
      "[ 500 /   57]         loss =  1.2541579008102417\n",
      "[1000 /   57]         loss =  1.4751263856887817\n",
      "[1500 /   57]         loss =  1.4640164375305176\n",
      "[2000 /   57]         loss =  1.2816063165664673\n",
      "[2500 /   57]         loss =  1.2610223293304443\n",
      "[3000 /   57]         loss =  1.2586520910263062\n",
      "[3500 /   57]         loss =  1.351928472518921\n",
      "[4000 /   57]         loss =  1.4501688480377197\n",
      "[4500 /   57]         loss =  1.4080801010131836\n",
      "[5000 /   57]         loss =  1.1650428771972656\n",
      "[5500 /   57]         loss =  1.4693161249160767\n",
      "[6000 /   57]         loss =  1.5839486122131348\n",
      "[6500 /   57]         loss =  1.2769495248794556\n",
      "[7000 /   57]         loss =  1.2751092910766602\n",
      "[7500 /   57]         loss =  1.472041130065918\n",
      "[8000 /   57]         loss =  1.1425535678863525\n",
      "[8500 /   57]         loss =  1.543561339378357\n",
      "[9000 /   57]         loss =  1.2586774826049805\n",
      "[9500 /   57]         loss =  1.281482219696045\n",
      "[10000 /   57]         loss =  1.492609977722168\n",
      "[10500 /   57]         loss =  1.4968987703323364\n",
      "[11000 /   57]         loss =  1.2715243101119995\n",
      "Epoch 58 model saved!\n",
      "--------------------epoch: 58\n",
      "[   0 /   58]         loss =  1.3687649965286255\n",
      "[ 500 /   58]         loss =  1.3881548643112183\n",
      "[1000 /   58]         loss =  1.2450488805770874\n",
      "[1500 /   58]         loss =  1.4289692640304565\n",
      "[2000 /   58]         loss =  1.3548994064331055\n",
      "[2500 /   58]         loss =  1.4605286121368408\n",
      "[3000 /   58]         loss =  1.288913607597351\n",
      "[3500 /   58]         loss =  1.2989987134933472\n",
      "[4000 /   58]         loss =  1.5070282220840454\n",
      "[4500 /   58]         loss =  1.322821855545044\n",
      "[5000 /   58]         loss =  1.3900794982910156\n",
      "[5500 /   58]         loss =  1.3201502561569214\n",
      "[6000 /   58]         loss =  1.0997138023376465\n",
      "[6500 /   58]         loss =  1.2806552648544312\n",
      "[7000 /   58]         loss =  1.4652369022369385\n",
      "[7500 /   58]         loss =  1.5838168859481812\n",
      "[8000 /   58]         loss =  1.3298349380493164\n",
      "[8500 /   58]         loss =  1.1708544492721558\n",
      "[9000 /   58]         loss =  1.4250116348266602\n",
      "[9500 /   58]         loss =  1.5976386070251465\n",
      "[10000 /   58]         loss =  1.2700815200805664\n",
      "[10500 /   58]         loss =  1.4794052839279175\n",
      "[11000 /   58]         loss =  1.5534067153930664\n",
      "--------------------epoch: 59\n",
      "[   0 /   59]         loss =  1.5728014707565308\n",
      "[ 500 /   59]         loss =  1.3319600820541382\n",
      "[1000 /   59]         loss =  1.1965583562850952\n",
      "[1500 /   59]         loss =  1.3161828517913818\n",
      "[2000 /   59]         loss =  1.1853474378585815\n",
      "[2500 /   59]         loss =  1.5137027502059937\n",
      "[3000 /   59]         loss =  1.3597739934921265\n",
      "[3500 /   59]         loss =  1.5303744077682495\n",
      "[4000 /   59]         loss =  1.4772989749908447\n",
      "[4500 /   59]         loss =  1.2521363496780396\n",
      "[5000 /   59]         loss =  1.6127738952636719\n",
      "[5500 /   59]         loss =  1.228426218032837\n",
      "[6000 /   59]         loss =  1.4087836742401123\n",
      "[6500 /   59]         loss =  1.4105110168457031\n",
      "[7000 /   59]         loss =  1.3358142375946045\n",
      "[7500 /   59]         loss =  1.2464994192123413\n",
      "[8000 /   59]         loss =  1.2920646667480469\n",
      "[8500 /   59]         loss =  1.4716054201126099\n",
      "[9000 /   59]         loss =  1.1827611923217773\n",
      "[9500 /   59]         loss =  1.5438623428344727\n",
      "[10000 /   59]         loss =  1.3481190204620361\n",
      "[10500 /   59]         loss =  1.2169437408447266\n",
      "[11000 /   59]         loss =  1.2117537260055542\n",
      "--------------------epoch: 60\n",
      "[   0 /   60]         loss =  1.1403926610946655\n",
      "[ 500 /   60]         loss =  1.2825413942337036\n",
      "[1000 /   60]         loss =  1.1795251369476318\n",
      "[1500 /   60]         loss =  1.4251264333724976\n",
      "[2000 /   60]         loss =  1.3739182949066162\n",
      "[2500 /   60]         loss =  1.3560256958007812\n",
      "[3000 /   60]         loss =  1.3478399515151978\n",
      "[3500 /   60]         loss =  1.6263607740402222\n",
      "[4000 /   60]         loss =  1.3284591436386108\n",
      "[4500 /   60]         loss =  1.1894316673278809\n",
      "[5000 /   60]         loss =  1.2338348627090454\n",
      "[5500 /   60]         loss =  1.270991325378418\n",
      "[6000 /   60]         loss =  1.534778356552124\n",
      "[6500 /   60]         loss =  1.4279934167861938\n",
      "[7000 /   60]         loss =  1.4935784339904785\n",
      "[7500 /   60]         loss =  1.4248758554458618\n",
      "[8000 /   60]         loss =  1.2724568843841553\n",
      "[8500 /   60]         loss =  1.4207335710525513\n",
      "[9000 /   60]         loss =  1.3203775882720947\n",
      "[9500 /   60]         loss =  1.1502400636672974\n",
      "[10000 /   60]         loss =  1.437042474746704\n",
      "[10500 /   60]         loss =  1.564895749092102\n",
      "[11000 /   60]         loss =  1.5037354230880737\n",
      "Epoch 61 model saved!\n",
      "--------------------epoch: 61\n",
      "[   0 /   61]         loss =  1.4732099771499634\n",
      "[ 500 /   61]         loss =  1.4124529361724854\n",
      "[1000 /   61]         loss =  1.228622555732727\n",
      "[1500 /   61]         loss =  1.3176145553588867\n",
      "[2000 /   61]         loss =  1.4560742378234863\n",
      "[2500 /   61]         loss =  1.3381282091140747\n",
      "[3000 /   61]         loss =  1.3094450235366821\n",
      "[3500 /   61]         loss =  1.3231440782546997\n",
      "[4000 /   61]         loss =  1.533848524093628\n",
      "[4500 /   61]         loss =  1.2884795665740967\n",
      "[5000 /   61]         loss =  1.513527274131775\n",
      "[5500 /   61]         loss =  1.3906697034835815\n",
      "[6000 /   61]         loss =  1.4506102800369263\n",
      "[6500 /   61]         loss =  1.3790199756622314\n",
      "[7000 /   61]         loss =  1.5545673370361328\n",
      "[7500 /   61]         loss =  1.2508256435394287\n",
      "[8000 /   61]         loss =  1.3712753057479858\n",
      "[8500 /   61]         loss =  1.3002214431762695\n",
      "[9000 /   61]         loss =  1.2055777311325073\n",
      "[9500 /   61]         loss =  1.4900580644607544\n",
      "[10000 /   61]         loss =  1.2094650268554688\n",
      "[10500 /   61]         loss =  1.3103997707366943\n",
      "[11000 /   61]         loss =  1.4452723264694214\n",
      "--------------------epoch: 62\n",
      "[   0 /   62]         loss =  1.3331661224365234\n",
      "[ 500 /   62]         loss =  1.3461076021194458\n",
      "[1000 /   62]         loss =  1.3849538564682007\n",
      "[1500 /   62]         loss =  1.246972680091858\n",
      "[2000 /   62]         loss =  1.199337363243103\n",
      "[2500 /   62]         loss =  1.3146461248397827\n",
      "[3000 /   62]         loss =  1.2861748933792114\n",
      "[3500 /   62]         loss =  1.1227636337280273\n",
      "[4000 /   62]         loss =  1.4381678104400635\n",
      "[4500 /   62]         loss =  1.5360299348831177\n",
      "[5000 /   62]         loss =  1.3544726371765137\n",
      "[5500 /   62]         loss =  1.3776954412460327\n",
      "[6000 /   62]         loss =  1.5583856105804443\n",
      "[6500 /   62]         loss =  1.302412509918213\n",
      "[7000 /   62]         loss =  1.2161093950271606\n",
      "[7500 /   62]         loss =  1.171635627746582\n",
      "[8000 /   62]         loss =  1.2675925493240356\n",
      "[8500 /   62]         loss =  1.251003623008728\n",
      "[9000 /   62]         loss =  1.4327247142791748\n",
      "[9500 /   62]         loss =  1.5661594867706299\n",
      "[10000 /   62]         loss =  1.2928413152694702\n",
      "[10500 /   62]         loss =  1.2844867706298828\n",
      "[11000 /   62]         loss =  1.4245623350143433\n",
      "--------------------epoch: 63\n",
      "[   0 /   63]         loss =  1.2584697008132935\n",
      "[ 500 /   63]         loss =  1.3623825311660767\n",
      "[1000 /   63]         loss =  1.3285659551620483\n",
      "[1500 /   63]         loss =  1.1460530757904053\n",
      "[2000 /   63]         loss =  1.043694257736206\n",
      "[2500 /   63]         loss =  1.220637559890747\n",
      "[3000 /   63]         loss =  1.2584245204925537\n",
      "[3500 /   63]         loss =  1.2850052118301392\n",
      "[4000 /   63]         loss =  1.265173077583313\n",
      "[4500 /   63]         loss =  1.268259882926941\n",
      "[5000 /   63]         loss =  1.3505886793136597\n",
      "[5500 /   63]         loss =  1.4117586612701416\n",
      "[6000 /   63]         loss =  1.335353970527649\n",
      "[6500 /   63]         loss =  1.13517427444458\n",
      "[7000 /   63]         loss =  1.4157682657241821\n",
      "[7500 /   63]         loss =  1.3997647762298584\n",
      "[8000 /   63]         loss =  1.5573346614837646\n",
      "[8500 /   63]         loss =  1.2488890886306763\n",
      "[9000 /   63]         loss =  1.568615436553955\n",
      "[9500 /   63]         loss =  1.2339709997177124\n",
      "[10000 /   63]         loss =  1.2326059341430664\n",
      "[10500 /   63]         loss =  1.0554051399230957\n",
      "[11000 /   63]         loss =  1.332406997680664\n",
      "Epoch 64 model saved!\n",
      "--------------------epoch: 64\n",
      "[   0 /   64]         loss =  1.4552476406097412\n",
      "[ 500 /   64]         loss =  1.3673088550567627\n",
      "[1000 /   64]         loss =  1.4000260829925537\n",
      "[1500 /   64]         loss =  1.1926097869873047\n",
      "[2000 /   64]         loss =  1.3815820217132568\n",
      "[2500 /   64]         loss =  1.343854308128357\n",
      "[3000 /   64]         loss =  1.406004786491394\n",
      "[3500 /   64]         loss =  1.23598313331604\n",
      "[4000 /   64]         loss =  1.564928412437439\n",
      "[4500 /   64]         loss =  1.4280362129211426\n",
      "[5000 /   64]         loss =  1.2129325866699219\n",
      "[5500 /   64]         loss =  1.3731269836425781\n",
      "[6000 /   64]         loss =  1.3678607940673828\n",
      "[6500 /   64]         loss =  1.4323521852493286\n",
      "[7000 /   64]         loss =  1.3793820142745972\n",
      "[7500 /   64]         loss =  1.3034313917160034\n",
      "[8000 /   64]         loss =  1.4753079414367676\n",
      "[8500 /   64]         loss =  1.4534125328063965\n",
      "[9000 /   64]         loss =  1.7082973718643188\n",
      "[9500 /   64]         loss =  1.3065745830535889\n",
      "[10000 /   64]         loss =  1.1404757499694824\n",
      "[10500 /   64]         loss =  1.3153014183044434\n",
      "[11000 /   64]         loss =  1.4883005619049072\n",
      "--------------------epoch: 65\n",
      "[   0 /   65]         loss =  1.5626945495605469\n",
      "[ 500 /   65]         loss =  1.3014652729034424\n",
      "[1000 /   65]         loss =  1.2881921529769897\n",
      "[1500 /   65]         loss =  1.3930394649505615\n",
      "[2000 /   65]         loss =  1.2325713634490967\n",
      "[2500 /   65]         loss =  1.2380588054656982\n",
      "[3000 /   65]         loss =  1.2254376411437988\n",
      "[3500 /   65]         loss =  1.3708446025848389\n",
      "[4000 /   65]         loss =  1.0727405548095703\n",
      "[4500 /   65]         loss =  1.3526722192764282\n",
      "[5000 /   65]         loss =  1.3582322597503662\n",
      "[5500 /   65]         loss =  1.3121535778045654\n",
      "[6000 /   65]         loss =  1.4893074035644531\n",
      "[6500 /   65]         loss =  1.2477831840515137\n",
      "[7000 /   65]         loss =  1.2261546850204468\n",
      "[7500 /   65]         loss =  1.2436093091964722\n",
      "[8000 /   65]         loss =  1.4386110305786133\n",
      "[8500 /   65]         loss =  1.1709295511245728\n",
      "[9000 /   65]         loss =  1.2568942308425903\n",
      "[9500 /   65]         loss =  1.2056694030761719\n",
      "[10000 /   65]         loss =  1.4547988176345825\n",
      "[10500 /   65]         loss =  1.3195909261703491\n",
      "[11000 /   65]         loss =  1.2339246273040771\n",
      "--------------------epoch: 66\n",
      "[   0 /   66]         loss =  1.0893588066101074\n",
      "[ 500 /   66]         loss =  1.1223400831222534\n",
      "[1000 /   66]         loss =  1.1799490451812744\n",
      "[1500 /   66]         loss =  1.0313879251480103\n",
      "[2000 /   66]         loss =  1.303247094154358\n",
      "[2500 /   66]         loss =  1.374070644378662\n",
      "[3000 /   66]         loss =  1.4528120756149292\n",
      "[3500 /   66]         loss =  1.2160866260528564\n",
      "[4000 /   66]         loss =  1.2758768796920776\n",
      "[4500 /   66]         loss =  1.3735063076019287\n",
      "[5000 /   66]         loss =  1.3817211389541626\n",
      "[5500 /   66]         loss =  1.093675136566162\n",
      "[6000 /   66]         loss =  1.548020362854004\n",
      "[6500 /   66]         loss =  1.1263813972473145\n",
      "[7000 /   66]         loss =  1.3344179391860962\n",
      "[7500 /   66]         loss =  1.205597162246704\n",
      "[8000 /   66]         loss =  1.476878046989441\n",
      "[8500 /   66]         loss =  1.1326913833618164\n",
      "[9000 /   66]         loss =  1.513724446296692\n",
      "[9500 /   66]         loss =  1.3585096597671509\n",
      "[10000 /   66]         loss =  1.4215202331542969\n",
      "[10500 /   66]         loss =  1.541429877281189\n",
      "[11000 /   66]         loss =  1.3512521982192993\n",
      "Epoch 67 model saved!\n",
      "--------------------epoch: 67\n",
      "[   0 /   67]         loss =  1.246261477470398\n",
      "[ 500 /   67]         loss =  1.2890921831130981\n",
      "[1000 /   67]         loss =  1.1141014099121094\n",
      "[1500 /   67]         loss =  1.2514463663101196\n",
      "[2000 /   67]         loss =  1.3386561870574951\n",
      "[2500 /   67]         loss =  1.1260005235671997\n",
      "[3000 /   67]         loss =  1.1897352933883667\n",
      "[3500 /   67]         loss =  1.1507792472839355\n",
      "[4000 /   67]         loss =  1.1763583421707153\n",
      "[4500 /   67]         loss =  1.2777650356292725\n",
      "[5000 /   67]         loss =  1.3653944730758667\n",
      "[5500 /   67]         loss =  1.1831330060958862\n",
      "[6000 /   67]         loss =  1.3999489545822144\n",
      "[6500 /   67]         loss =  1.0919466018676758\n",
      "[7000 /   67]         loss =  1.0919395685195923\n",
      "[7500 /   67]         loss =  1.2587919235229492\n",
      "[8000 /   67]         loss =  1.4348537921905518\n",
      "[8500 /   67]         loss =  1.307509422302246\n",
      "[9000 /   67]         loss =  1.2345654964447021\n",
      "[9500 /   67]         loss =  1.0978577136993408\n",
      "[10000 /   67]         loss =  1.2679781913757324\n",
      "[10500 /   67]         loss =  1.3833727836608887\n",
      "[11000 /   67]         loss =  1.335095763206482\n",
      "--------------------epoch: 68\n",
      "[   0 /   68]         loss =  1.3470982313156128\n",
      "[ 500 /   68]         loss =  1.4217811822891235\n",
      "[1000 /   68]         loss =  1.3343340158462524\n",
      "[1500 /   68]         loss =  1.2996115684509277\n",
      "[2000 /   68]         loss =  1.1733068227767944\n",
      "[2500 /   68]         loss =  1.22527015209198\n",
      "[3000 /   68]         loss =  1.3298074007034302\n",
      "[3500 /   68]         loss =  1.359601616859436\n",
      "[4000 /   68]         loss =  1.1527025699615479\n",
      "[4500 /   68]         loss =  1.302578091621399\n",
      "[5000 /   68]         loss =  1.226830244064331\n",
      "[5500 /   68]         loss =  1.3877739906311035\n",
      "[6000 /   68]         loss =  1.2214782238006592\n",
      "[6500 /   68]         loss =  1.3398903608322144\n",
      "[7000 /   68]         loss =  1.2517619132995605\n",
      "[7500 /   68]         loss =  1.3367936611175537\n",
      "[8000 /   68]         loss =  1.1827507019042969\n",
      "[8500 /   68]         loss =  1.2189692258834839\n",
      "[9000 /   68]         loss =  1.3813587427139282\n",
      "[9500 /   68]         loss =  1.2191247940063477\n",
      "[10000 /   68]         loss =  1.1353816986083984\n",
      "[10500 /   68]         loss =  1.32895028591156\n",
      "[11000 /   68]         loss =  1.3816629648208618\n",
      "--------------------epoch: 69\n",
      "[   0 /   69]         loss =  1.2929655313491821\n",
      "[ 500 /   69]         loss =  1.3413972854614258\n",
      "[1000 /   69]         loss =  1.0474525690078735\n",
      "[1500 /   69]         loss =  1.307003140449524\n",
      "[2000 /   69]         loss =  1.2604936361312866\n",
      "[2500 /   69]         loss =  1.2577751874923706\n",
      "[3000 /   69]         loss =  1.1210906505584717\n",
      "[3500 /   69]         loss =  1.234159231185913\n",
      "[4000 /   69]         loss =  1.4018127918243408\n",
      "[4500 /   69]         loss =  1.4607080221176147\n",
      "[5000 /   69]         loss =  1.1852718591690063\n",
      "[5500 /   69]         loss =  1.2364660501480103\n",
      "[6000 /   69]         loss =  1.3470613956451416\n",
      "[6500 /   69]         loss =  1.4090944528579712\n",
      "[7000 /   69]         loss =  1.2493548393249512\n",
      "[7500 /   69]         loss =  1.4000428915023804\n",
      "[8000 /   69]         loss =  1.438823938369751\n",
      "[8500 /   69]         loss =  1.3418009281158447\n",
      "[9000 /   69]         loss =  1.3565642833709717\n",
      "[9500 /   69]         loss =  1.3350001573562622\n",
      "[10000 /   69]         loss =  1.434910535812378\n",
      "[10500 /   69]         loss =  1.470917820930481\n",
      "[11000 /   69]         loss =  1.395383596420288\n",
      "Epoch 70 model saved!\n",
      "--------------------epoch: 70\n",
      "[   0 /   70]         loss =  1.2278156280517578\n",
      "[ 500 /   70]         loss =  1.2678512334823608\n",
      "[1000 /   70]         loss =  1.2679797410964966\n",
      "[1500 /   70]         loss =  1.3574458360671997\n",
      "[2000 /   70]         loss =  1.0939133167266846\n",
      "[2500 /   70]         loss =  1.3165837526321411\n",
      "[3000 /   70]         loss =  1.2990858554840088\n",
      "[3500 /   70]         loss =  1.181108832359314\n",
      "[4000 /   70]         loss =  1.0543978214263916\n",
      "[4500 /   70]         loss =  1.5358822345733643\n",
      "[5000 /   70]         loss =  1.3417866230010986\n",
      "[5500 /   70]         loss =  1.376579761505127\n",
      "[6000 /   70]         loss =  1.2770124673843384\n",
      "[6500 /   70]         loss =  1.4858113527297974\n",
      "[7000 /   70]         loss =  1.3042011260986328\n",
      "[7500 /   70]         loss =  1.3519675731658936\n",
      "[8000 /   70]         loss =  1.0391762256622314\n",
      "[8500 /   70]         loss =  1.183458685874939\n",
      "[9000 /   70]         loss =  1.313890814781189\n",
      "[9500 /   70]         loss =  1.1749008893966675\n",
      "[10000 /   70]         loss =  1.1800227165222168\n",
      "[10500 /   70]         loss =  0.9345101118087769\n",
      "[11000 /   70]         loss =  1.1598349809646606\n",
      "--------------------epoch: 71\n",
      "[   0 /   71]         loss =  0.9820790886878967\n",
      "[ 500 /   71]         loss =  1.2251896858215332\n",
      "[1000 /   71]         loss =  1.3622583150863647\n",
      "[1500 /   71]         loss =  1.0372533798217773\n",
      "[2000 /   71]         loss =  1.4230620861053467\n",
      "[2500 /   71]         loss =  1.025607943534851\n",
      "[3000 /   71]         loss =  1.2863935232162476\n",
      "[3500 /   71]         loss =  1.3323084115982056\n",
      "[4000 /   71]         loss =  1.2248976230621338\n",
      "[4500 /   71]         loss =  1.1826788187026978\n",
      "[5000 /   71]         loss =  1.3774994611740112\n",
      "[5500 /   71]         loss =  1.274401068687439\n",
      "[6000 /   71]         loss =  1.2358003854751587\n",
      "[6500 /   71]         loss =  1.2905926704406738\n",
      "[7000 /   71]         loss =  1.3289000988006592\n",
      "[7500 /   71]         loss =  1.1799139976501465\n",
      "[8000 /   71]         loss =  1.0912401676177979\n",
      "[8500 /   71]         loss =  1.3581147193908691\n",
      "[9000 /   71]         loss =  1.3202297687530518\n",
      "[9500 /   71]         loss =  1.2077778577804565\n",
      "[10000 /   71]         loss =  1.3418641090393066\n",
      "[10500 /   71]         loss =  1.138378381729126\n",
      "[11000 /   71]         loss =  1.4138108491897583\n",
      "--------------------epoch: 72\n",
      "[   0 /   72]         loss =  1.290534496307373\n",
      "[ 500 /   72]         loss =  1.4124205112457275\n",
      "[1000 /   72]         loss =  1.3048912286758423\n",
      "[1500 /   72]         loss =  1.385703682899475\n",
      "[2000 /   72]         loss =  1.1689666509628296\n",
      "[2500 /   72]         loss =  1.3789868354797363\n",
      "[3000 /   72]         loss =  1.2661044597625732\n",
      "[3500 /   72]         loss =  1.2948733568191528\n",
      "[4000 /   72]         loss =  1.2110068798065186\n",
      "[4500 /   72]         loss =  1.153084397315979\n",
      "[5000 /   72]         loss =  1.2180589437484741\n",
      "[5500 /   72]         loss =  1.2678412199020386\n",
      "[6000 /   72]         loss =  1.4353899955749512\n",
      "[6500 /   72]         loss =  1.4835401773452759\n",
      "[7000 /   72]         loss =  1.1777386665344238\n",
      "[7500 /   72]         loss =  1.1484609842300415\n",
      "[8000 /   72]         loss =  1.3922702074050903\n",
      "[8500 /   72]         loss =  1.377130150794983\n",
      "[9000 /   72]         loss =  1.3298933506011963\n",
      "[9500 /   72]         loss =  1.375466227531433\n",
      "[10000 /   72]         loss =  1.1785696744918823\n",
      "[10500 /   72]         loss =  1.1995398998260498\n",
      "[11000 /   72]         loss =  1.4182190895080566\n",
      "Epoch 73 model saved!\n",
      "--------------------epoch: 73\n",
      "[   0 /   73]         loss =  1.2450523376464844\n",
      "[ 500 /   73]         loss =  1.487578272819519\n",
      "[1000 /   73]         loss =  1.3873409032821655\n",
      "[1500 /   73]         loss =  1.2628154754638672\n",
      "[2000 /   73]         loss =  1.2183116674423218\n",
      "[2500 /   73]         loss =  1.1861828565597534\n",
      "[3000 /   73]         loss =  1.2568728923797607\n",
      "[3500 /   73]         loss =  1.1644721031188965\n",
      "[4000 /   73]         loss =  1.4011452198028564\n",
      "[4500 /   73]         loss =  1.164528727531433\n",
      "[5000 /   73]         loss =  1.2411466836929321\n",
      "[5500 /   73]         loss =  1.2702716588974\n",
      "[6000 /   73]         loss =  1.1385740041732788\n",
      "[6500 /   73]         loss =  1.3414576053619385\n",
      "[7000 /   73]         loss =  1.149488091468811\n",
      "[7500 /   73]         loss =  1.230989694595337\n",
      "[8000 /   73]         loss =  1.1960670948028564\n",
      "[8500 /   73]         loss =  1.4218965768814087\n",
      "[9000 /   73]         loss =  1.3201850652694702\n",
      "[9500 /   73]         loss =  1.3366990089416504\n",
      "[10000 /   73]         loss =  1.1951658725738525\n",
      "[10500 /   73]         loss =  1.2430720329284668\n",
      "[11000 /   73]         loss =  1.2112257480621338\n",
      "--------------------epoch: 74\n",
      "[   0 /   74]         loss =  1.345335841178894\n",
      "[ 500 /   74]         loss =  1.2036986351013184\n",
      "[1000 /   74]         loss =  1.375412940979004\n",
      "[1500 /   74]         loss =  1.2605396509170532\n",
      "[2000 /   74]         loss =  1.179659366607666\n",
      "[2500 /   74]         loss =  1.3302724361419678\n",
      "[3000 /   74]         loss =  1.4859317541122437\n",
      "[3500 /   74]         loss =  1.3244575262069702\n",
      "[4000 /   74]         loss =  1.2964107990264893\n",
      "[4500 /   74]         loss =  1.2461129426956177\n",
      "[5000 /   74]         loss =  1.226729154586792\n",
      "[5500 /   74]         loss =  1.2552987337112427\n",
      "[6000 /   74]         loss =  1.1151295900344849\n",
      "[6500 /   74]         loss =  1.1592379808425903\n",
      "[7000 /   74]         loss =  1.391776442527771\n",
      "[7500 /   74]         loss =  1.2510896921157837\n",
      "[8000 /   74]         loss =  1.315763235092163\n",
      "[8500 /   74]         loss =  1.1475781202316284\n",
      "[9000 /   74]         loss =  1.3187190294265747\n",
      "[9500 /   74]         loss =  1.2708096504211426\n",
      "[10000 /   74]         loss =  1.1672611236572266\n",
      "[10500 /   74]         loss =  1.0930081605911255\n",
      "[11000 /   74]         loss =  1.1317089796066284\n"
     ]
    }
   ],
   "source": [
    "# AWGN-CE-Stage2\n",
    "!python3 Trainng_SemanticRL.py --training_config /kaggle/working/SemanticRL/config/config_AWGN_CE_Stage2.yaml --dataset_path /kaggle/working/SemanticRL/dataset --log_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:19:36.432801Z",
     "iopub.status.busy": "2025-08-12T11:19:36.432275Z",
     "iopub.status.idle": "2025-08-12T11:19:36.440418Z",
     "shell.execute_reply": "2025-08-12T11:19:36.439710Z",
     "shell.execute_reply.started": "2025-08-12T11:19:36.432773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/Evaluation/Run_Inference_Checkpoints.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/Evaluation/Run_Inference_Checkpoints.py\n",
    "\"\"\"\n",
    "This work is created by KunLu. Copyright reserved.\n",
    "lukun199@gmail.com\n",
    "19th Feb., 2021\n",
    "\n",
    "# Inference.py\n",
    "\"\"\"\n",
    "import os, platform, json, time, argparse, random, sys\n",
    "sys.path.append('./')\n",
    "import torch\n",
    "from math import log\n",
    "from data_loader import Dataset_sentence_test, collate_func\n",
    "from model import LSTMEncoder, LSTMDecoder, Embeds\n",
    "from utils import Normlize_tx, Channel, smaple_n_times\n",
    "import torch.utils.data as data\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "def onetime_sample_max(ckpt_this, input_data, encoder, decoder, normlize_layer, channel, len_batch, save_dir, eval_times=1):\n",
    "    result_strs = []\n",
    "    gt_strs = []\n",
    "    to_save_dict = {}\n",
    "    minib = 2048\n",
    "    input_data_list = input_data.split(minib)\n",
    "    len_batch_list = len_batch.split(minib)\n",
    "    out_chunk = []\n",
    "    gt_chunk = []\n",
    "    for idxx, train_sent in enumerate(input_data_list):\n",
    "        train_sent = train_sent[:,1:]\n",
    "        train_sent, train_len = collate_func((zip(train_sent, len_batch_list[idxx]-1)))  # len minus 1 for LSTM\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, _ = encoder(train_sent, train_len)\n",
    "            output = normlize_layer.apply(output)\n",
    "            output = smaple_n_times(eval_times, output)\n",
    "            if args.channel == 'gaussian':\n",
    "                output = channel.awgn(output, _snr=_snr)\n",
    "            elif args.channel == 'fading':\n",
    "                output = channel.phase_invariant_fading(output, _snr=_snr)\n",
    "            elif args.channel == 'vary_gaussian':\n",
    "                output = channel.awgn(output, _snr=_snr + random.uniform(-10, 10))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            output = decoder.sample_max_batch(output, x_mask=None)\n",
    "\n",
    "        out_chunk.append(output.cpu().numpy().tolist())\n",
    "        gt_chunk.append(smaple_n_times(eval_times, train_sent).cpu().numpy().tolist())\n",
    "\n",
    "        if idxx % 4 ==0: print('processing {}'.format(idxx*minib))\n",
    "\n",
    "    for xx in range(len(out_chunk)):\n",
    "        for x in range(len(out_chunk[xx])):\n",
    "            gt_strs.append(gt_chunk[xx][x])\n",
    "            result_strs.append(out_chunk[xx][x])\n",
    "\n",
    "    to_save_dict['gt_strs'] = gt_strs\n",
    "    to_save_dict['result_strs'] = result_strs\n",
    "\n",
    "    json.dump(to_save_dict, open(save_dir + '/ckpt{}.json'.format(ckpt_this), 'w'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    _snr = 10\n",
    "    _iscomplex = True\n",
    "    channel_dim = 256\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--path\", type=str, default='./ckpt_AWGN_RL')\n",
    "    parser.add_argument(\"--name\", type=str, default='Test_AWGN_RL')\n",
    "    parser.add_argument(\"--data_root\", type=str, default='H:\\MASTER')\n",
    "    parser.add_argument(\"--epoch_start\", type=int, default=72)\n",
    "    parser.add_argument(\"--epoch_end\", type=int, default=73)\n",
    "    parser.add_argument(\"--channel\", type=str, default='gaussian')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_path = os.path.join(args.data_root)\n",
    "    data_train = Dataset_sentence_test(_path=data_path)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # use CPU or GPU\n",
    "    embeds_shared = Embeds(vocab_size=data_train.get_dict_len(), num_hidden=128).to(device)\n",
    "    encoder = LSTMEncoder(channel_dim=channel_dim, embedds=embeds_shared).to(device)\n",
    "    decoder = LSTMDecoder(channel_dim=channel_dim, embedds=embeds_shared, vocab_size=data_train.get_dict_len()).to(\n",
    "        device)\n",
    "\n",
    "    encoder = encoder.eval()\n",
    "    decoder = decoder.eval()\n",
    "    embeds_shared = embeds_shared.eval()\n",
    "\n",
    "    normlize_layer = Normlize_tx(_iscomplex=_iscomplex)\n",
    "    channel = Channel(_iscomplex=_iscomplex)\n",
    "\n",
    "\n",
    "    #for rl training\n",
    "    ckpt_dir = args.path\n",
    "    save_dir = './Evaluation/InferenceResutls/{}'.format(args.name)\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "    all_data = torch.tensor(data_train.data_num).to(device)\n",
    "    len_batch = torch.tensor(list(map(lambda s: sum(s != 0), all_data))).to(device)\n",
    "\n",
    "    ss = time.time()\n",
    "\n",
    "    for idx in range(args.epoch_start, args.epoch_end):\n",
    "\n",
    "        if idx%3 ==0 and not os.path.exists(save_dir + '/ckpt{}.json'.format(idx)):\n",
    "\n",
    "            ckpt_this = idx\n",
    "\n",
    "            print('processing epoch {}'.format(ckpt_this))\n",
    "            model_path = '_epoch{}.pth'.format(ckpt_this)\n",
    "\n",
    "            encoder.load_state_dict(torch.load(ckpt_dir + '/encoder' + model_path))##,  map_location='cpu'))\n",
    "            decoder.load_state_dict(torch.load(ckpt_dir + '/decoder' + model_path))#,  map_location='cpu'))\n",
    "            embeds_shared.load_state_dict(torch.load(ckpt_dir + '/embeds_shared' + model_path))#,  map_location='cpu'))\n",
    "\n",
    "            onetime_sample_max(ckpt_this, all_data, encoder, decoder, normlize_layer, channel, len_batch, save_dir)\n",
    "\n",
    "            print('total time cost at {}: '.format(ckpt_this), time.time()-ss)\n",
    "        else:\n",
    "            print('skipping ckpt', idx)\n",
    "\n",
    "    print('done_all!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python3 /kaggle/working/SemanticRL/Evaluation/Run_Inference_Checkpoints.py --path /kaggle/working/SemanticRL/ckpt_AWGN_CE_Stage2 --name bleunotRL --data_root /kaggle/working/SemanticRL/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate total time of running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:22:55.637509Z",
     "iopub.status.busy": "2025-08-12T11:22:55.636834Z",
     "iopub.status.idle": "2025-08-12T11:27:32.492912Z",
     "shell.execute_reply": "2025-08-12T11:27:32.491728Z",
     "shell.execute_reply.started": "2025-08-12T11:22:55.637484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]------------vocabulary size is:---- 24064\n",
      "[*]------------sentences size is:---- 177398\n",
      "processing epoch 72\n",
      "processing 0\n",
      "processing 8192\n",
      "processing 16384\n",
      "processing 24576\n",
      "processing 32768\n",
      "processing 40960\n",
      "processing 49152\n",
      "processing 57344\n",
      "processing 65536\n",
      "processing 73728\n",
      "processing 81920\n",
      "processing 90112\n",
      "processing 98304\n",
      "processing 106496\n",
      "processing 114688\n",
      "processing 122880\n",
      "processing 131072\n",
      "processing 139264\n",
      "processing 147456\n",
      "processing 155648\n",
      "processing 163840\n",
      "processing 172032\n",
      "total time cost at 72:  227.89959526062012\n",
      "done_all!\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/SemanticRL/Evaluation/Run_Inference_Checkpoints.py --path /kaggle/input/model-bleu-notrl/kaggle/working/SemanticRL/ckpt_AWGN_CE_Stage2 --name bleunotRL --data_root /kaggle/working/SemanticRL/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:32:00.353345Z",
     "iopub.status.busy": "2025-08-12T11:32:00.352877Z",
     "iopub.status.idle": "2025-08-12T11:32:00.360443Z",
     "shell.execute_reply": "2025-08-12T11:32:00.359848Z",
     "shell.execute_reply.started": "2025-08-12T11:32:00.353313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/Evaluation/CaptionMetrics-master/Eval_Metric_Checkpoints.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/Evaluation/CaptionMetrics-master/Eval_Metric_Checkpoints.py\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "import json, os, argparse\n",
    "\n",
    "\n",
    "\n",
    "def proc_and_to_list(input):\n",
    "    res = []\n",
    "    tmp = ''\n",
    "    for x in input:\n",
    "        if x != 0 and x!= 2:\n",
    "            tmp+=(' '+str(x))\n",
    "    res.append(tmp[1:])\n",
    "    return res\n",
    "\n",
    "\n",
    "def cal_fun():\n",
    "    \"\"\"specify the path of json file, then calculate the metrics.\"\"\"\n",
    "\n",
    "    save_path = './Evaluation/EvalResults/{}'.format(args.name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for x in range(args.epoch_start,args.epoch_end):\n",
    "        if os.path.exists('{}/ckpt{}.json'.format(args.path, x)) \\\n",
    "                and not os.path.exists(save_path + '/scores_ckpt{}.json'.format(x)):\n",
    "            score_dict = {}\n",
    "            score_bleu_dict, score_cider_dict, score_rouge_dict = {}, {}, {}\n",
    "\n",
    "            with open('{}/ckpt{}.json'.format(args.path, x), 'rb') as file:\n",
    "                tmp = json.load(file)\n",
    "                gts, res = {idx: proc_and_to_list(cont) for idx, cont in enumerate(tmp['gt_strs'])}, \\\n",
    "                           {idx: proc_and_to_list(cont) for idx, cont in enumerate(tmp['result_strs'])}\n",
    "            score_bleu, score_cider, score_rouge = main(gts, res)\n",
    "            score_bleu_dict[x] = score_bleu\n",
    "            score_cider_dict[x] = score_cider\n",
    "            score_rouge_dict[x] = score_rouge\n",
    "\n",
    "            score_dict['bleu'] = score_bleu_dict\n",
    "            score_dict['cider'] = score_cider_dict\n",
    "            score_dict['rouge'] = score_rouge_dict\n",
    "\n",
    "            json.dump(score_dict, open(save_path + '/scores_ckpt{}.json'.format(x), 'w'))\n",
    "        else:\n",
    "            print('-----[*]-----skipping ckpt at ', x)\n",
    "\n",
    "\n",
    "def bleu(gts, res):\n",
    "    scorer = Bleu(n=4)\n",
    "    score, scores = scorer.compute_score(gts, res)\n",
    "    print('belu = %s' % score)\n",
    "    return score\n",
    "\n",
    "def cider(gts, res):\n",
    "    scorer = Cider()\n",
    "    # scorer += (hypo[0], ref1)\n",
    "    (score, scores) = scorer.compute_score(gts, res)\n",
    "    print('cider = %s' % score)\n",
    "    return score\n",
    "\n",
    "def meteor(gts, res):\n",
    "    scorer = Meteor()\n",
    "    score, scores = scorer.compute_score(gts, res)\n",
    "    print('meter = %s' % score)\n",
    "    return score\n",
    "\n",
    "def rouge(gts, res):\n",
    "    scorer = Rouge()\n",
    "    score, scores = scorer.compute_score(gts, res)\n",
    "    print('rouge = %s' % score)\n",
    "    return score\n",
    "\n",
    "def spice(gts, res):\n",
    "    scorer = Spice()\n",
    "    score, scores = scorer.compute_score(gts, res)\n",
    "    print('spice = %s' % score)\n",
    "    return score\n",
    "\n",
    "def main(gts, res):\n",
    "    score_bleu = bleu(gts, res)\n",
    "    score_cider = cider(gts, res)\n",
    "    #meteor(gts, res)\n",
    "    score_rouge = rouge(gts, res)\n",
    "    #spice(gts, res)\n",
    "    return score_bleu, score_cider, score_rouge\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--path\", type=str, default='')\n",
    "    parser.add_argument(\"--name\", type=str, default='')\n",
    "    parser.add_argument(\"--epoch_start\", type=int, default=72)\n",
    "    parser.add_argument(\"--epoch_end\", type=int, default=73)\n",
    "    args = parser.parse_args()\n",
    "    cal_fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T11:32:04.580862Z",
     "iopub.status.busy": "2025-08-12T11:32:04.580606Z",
     "iopub.status.idle": "2025-08-12T11:33:35.262526Z",
     "shell.execute_reply": "2025-08-12T11:33:35.261597Z",
     "shell.execute_reply.started": "2025-08-12T11:32:04.580843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 2349698, 'reflen': 2324783, 'guess': [2349698, 2172300, 1994902, 1817504], 'correct': [8929, 111, 2, 0]}\n",
      "ratio: 1.0107171292976587\n",
      "belu = [0.003800062816583236, 0.0004406532485714403, 5.795631898771724e-05, 3.2170421546079057e-09]\n",
      "cider = 0.009078183307135999\n",
      "rouge = 0.00532383406820812\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/SemanticRL/Evaluation/CaptionMetrics-master/Eval_Metric_Checkpoints.py --path /kaggle/working/SemanticRL/Evaluation/InferenceResutls/bleunotRL --name ckpt72.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T17:11:20.825984Z",
     "iopub.status.busy": "2025-08-04T17:11:20.825377Z",
     "iopub.status.idle": "2025-08-04T17:11:20.832713Z",
     "shell.execute_reply": "2025-08-04T17:11:20.832052Z",
     "shell.execute_reply.started": "2025-08-04T17:11:20.825952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SemanticRL/Evaluation/Inference_Given_Input.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SemanticRL/Evaluation/Inference_Given_Input.py\n",
    "\"\"\"\n",
    "This work is created by KunLu. Copyright reserved.\n",
    "lukun199@gmail.com\n",
    "19th Feb., 2021\n",
    "\n",
    "# Inference.py\n",
    "\"\"\"\n",
    "import os, platform, json, time, pickle, sys, argparse\n",
    "import torch\n",
    "from math import log\n",
    "sys.path.append('./')\n",
    "from data_loader import Dataset_sentence_test, collate_func\n",
    "from model import LSTMEncoder, LSTMDecoder, Embeds\n",
    "from utils import Normlize_tx, Channel, smaple_n_times\n",
    "\n",
    "\n",
    "_snr = 10\n",
    "_iscomplex = True\n",
    "channel_dim = 256\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu:0\")\n",
    "#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # use CPU or GPU\n",
    "embeds_shared = Embeds(vocab_size=24064, num_hidden=128).to(device)\n",
    "encoder = LSTMEncoder(channel_dim=channel_dim, embedds=embeds_shared).to(device)\n",
    "decoder = LSTMDecoder(channel_dim=channel_dim, embedds=embeds_shared, vocab_size = 24064).to(device)\n",
    "\n",
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()\n",
    "embeds_shared = embeds_shared.eval()\n",
    "\n",
    "\n",
    "normlize_layer = Normlize_tx(_iscomplex=_iscomplex)\n",
    "channel = Channel(_iscomplex=_iscomplex)\n",
    "\n",
    "\n",
    "def do_test(input_data, encoder, decoder, normlize_layer, channel, len_batch):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = encoder(input_data, len_batch)\n",
    "        output = normlize_layer.apply(output)\n",
    "        output = channel.awgn(output, _snr=_snr)\n",
    "        output = decoder.sample_max_batch(output, None)\n",
    "\n",
    "    return output\n",
    "\n",
    "SemanticRL_example = ['this is a typical sentence used to check the performance',\n",
    "                        'this is a typical unk used to check the performance',\n",
    "                      'this is exactly a long sentence with complex structure which might be a challenge for both',\n",
    "                       'i have just brought a yellow banana',\n",
    "                      'a man is holding a giant elephant on his hand',\n",
    "                      ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ckpt_pathCE\", type=str, default='./ckpt_AWGN_CE_Stage2')\n",
    "    parser.add_argument(\"--ckpt_pathRL\", type=str, default='./ckpt_AWGN_RL_SemanticRLv1')  # or './ckpt_AWGN_RL'\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dict_train = pickle.load(open('./train_dict.pkl', 'rb'))\n",
    "    rev_dict = {vv: kk for kk, vv in dict_train.items()}\n",
    "    \n",
    "    for input_str in SemanticRL_example:\n",
    "\n",
    "        input_vector = [dict_train[x] for x in input_str.split(' ')] + [2]\n",
    "        input_len = len(input_vector)\n",
    "        input_vector = torch.tensor(input_vector)\n",
    "    \n",
    "        ckpt_dir = args.ckpt_pathCE\n",
    "        model_name = os.path.basename(ckpt_dir)\n",
    "    \n",
    "        encoder.load_state_dict(torch.load(ckpt_dir + '/encoder_epoch72.pth', map_location='cpu'))\n",
    "        decoder.load_state_dict(torch.load(ckpt_dir + '/decoder_epoch72.pth', map_location='cpu'))\n",
    "        embeds_shared.load_state_dict(torch.load(ckpt_dir + '/embeds_shared_epoch72.pth',  map_location='cpu'))\n",
    "    \n",
    "        for _ in range(5):\n",
    "            output = do_test(input_vector.unsqueeze(0), encoder, decoder, normlize_layer, channel,\n",
    "                    len_batch=torch.tensor(input_len).view(-1, ))\n",
    "            output = output.cpu().numpy()[0]\n",
    "            res = ' '.join(rev_dict[x] for x in output if x!=0 and x!=2)  # remove 'PAD' and 'EOS'\n",
    "            print('result of {}:            {}'.format(model_name, res))\n",
    "        print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T17:11:23.689067Z",
     "iopub.status.busy": "2025-08-04T17:11:23.688807Z",
     "iopub.status.idle": "2025-08-04T17:11:26.930627Z",
     "shell.execute_reply": "2025-08-04T17:11:26.929922Z",
     "shell.execute_reply.started": "2025-08-04T17:11:23.689048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of ckpt_AWGN_CE_Stage2:            this is a typical mep again to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical sentence opportunity to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical sentence opportunity to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical sentence opportunity to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical sentence opportunity to check the performance\n",
      "--------------------------------------------------\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical to continue to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical success debate to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical order is to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical in cases to check the performance\n",
      "result of ckpt_AWGN_CE_Stage2:            this is a typical to continue to check the performance\n",
      "--------------------------------------------------\n",
      "result of ckpt_AWGN_CE_Stage2:            this is completely a long item is exactly what could it be a challenge for both\n",
      "result of ckpt_AWGN_CE_Stage2:            this is exactly a long direction with nato a product it are a challenge for political\n",
      "result of ckpt_AWGN_CE_Stage2:            this is completely a long event to compare better targets i are doing up for both\n",
      "result of ckpt_AWGN_CE_Stage2:            this is exactly a long direction and the crisis package which are required which with both\n",
      "result of ckpt_AWGN_CE_Stage2:            this is exactly a long mandate with normal crisis was which are doing but with both\n",
      "--------------------------------------------------\n",
      "result of ckpt_AWGN_CE_Stage2:            i have just brought a round forward\n",
      "result of ckpt_AWGN_CE_Stage2:            i have just brought a plea forward\n",
      "result of ckpt_AWGN_CE_Stage2:            i have just brought a plea twice\n",
      "result of ckpt_AWGN_CE_Stage2:            i have just brought a plea twice\n",
      "result of ckpt_AWGN_CE_Stage2:            i have just brought a plea out\n",
      "--------------------------------------------------\n",
      "result of ckpt_AWGN_CE_Stage2:            a man is getting a giant leap on his hand\n",
      "result of ckpt_AWGN_CE_Stage2:            a man is holding a giant criterion on his hand\n",
      "result of ckpt_AWGN_CE_Stage2:            a man is holding a giant condition on his hand\n",
      "result of ckpt_AWGN_CE_Stage2:            a man is holding a delicate route on his hand\n",
      "result of ckpt_AWGN_CE_Stage2:            a man is holding a giant trap on his hand\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/SemanticRL/Evaluation/Inference_Given_Input.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SemanticRL-SCSIU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python3 /kaggle/working/SemanticRL/Trainng_SemanticRL.py --training_config /kaggle/working/SemanticRL/config/config_AWGN_RL_SCSIU.yaml --dataset_path /kaggle/working/SemanticRL/dataset"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7983453,
     "sourceId": 12634209,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8055477,
     "sourceId": 12743192,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8055497,
     "sourceId": 12743218,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 418283,
     "modelInstanceId": 399991,
     "sourceId": 503453,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
